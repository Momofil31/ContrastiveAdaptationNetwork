{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA2kWKxcvq2P"
      },
      "source": [
        "# Deep Learning Course Project\n",
        "## Students:\n",
        "* ### Filippo Momesso - filippo.momesso@studenti.unitn.it\n",
        "* ### Thomas De Min - thomas.demin@studenti.unitn.it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bLAmXRIiIfY"
      },
      "source": [
        "## Introduction\n",
        "This year's (A.Y. 2021/2022) Deep Learning course's project involves the topic of Unsupervised Domain Adaptation (UDA). We have been provided an UDA dataset consisting of two domains: <u>Real World</u> and <u>Product</u>. The objective is to \"propose a UDA technique to counteract the negative impact of the domain gap\". \n",
        "\n",
        "### Dataset\n",
        "The dataset [Adaptiope](https://ieeexplore.ieee.org/document/9423412) counts 123 classes but, in this case, only 20 of them will be investigated, each of them is balanced. Indeed, each class is made of 100 samples. As requested by the assignment we used a 80% train/test split.\n",
        "\n",
        "### Delivery\n",
        "In order to deliver a competitive solution in the real world, as a Deep Learning's project, we decided to dig into the literature of Unsupervised Domain Adaptation. Moreover, we also investigated the results of some of the recent techniques on the provided dataset.\n",
        "\n",
        "We decided to deliver:\n",
        "* A **baseline** implementation, that involves a ResNet18 fine-tuned on the source domain and tested on the target domain, in order to investigate upper and lower bound on the accuracy;\n",
        "* The implementation of the [**Contrastive Adaptation Network (CAN)**](https://arxiv.org/abs/1901.00976), one of the state-of-the-art techniques in the field and the most promising approach for Adaptiope Dataset;\n",
        "* Our **improved version** of CAN.\n",
        "\n",
        "[Here](https://wandb.ai/229356_229298/DL2022_229356_229298) you can find our Weight and Biases project with all the plots we showed and additional metrics and informations, like some wrongly predicted images.\n",
        "\n",
        "> Note: In all these scenarios a ResNet18 has been employed as backbone model.\n",
        "\n",
        "### Experiments\n",
        "For each approach we performed the UDA experiment in both directions, one at a time.\n",
        "* For the **baseline** approach we trained on Products and tested on Real World test set, in order to get a lower bound accuracy for the domain adaptation task. Vice-versa for Real World to Products. We also trained the network on both training sets and tested on their corresponding test set in order to get the upper bound accuracy.\n",
        "* For the implementation of **CAN** and our further **improvements** we procedeed in a similar way, but we omitted the computation of the upper bound.\n",
        "  \n",
        "Since each approach has its own requirements we trained all approaches with 30 epochs, except for CAN vanilla which required 60 epochs in order to converge. We kept as best model the one with the highest accuracy on the test set. For reproducible results, train and test splits are handled by a Generator with seed 0.\n",
        "\n",
        "> Note: We would have liked to train also the other approaches with 60 epochs, but it was time consuming in terms of GPU. \n",
        "\n",
        "### Requirements\n",
        "* We expect to load the datasets from a directory positioned in the root of Google Drive, named \"datasets\". The file must be named `Adaptiope.zip` (default name). Basically we will load the dataset from the following path `/content/drive/MyDrive/datasets/Adaptiope.zip `\n",
        "* By default weights are not saved throughout epochs but, if you would like to do so, please provide a directory in the root of Google Drive named \"weights\". Moreover, turn the save flag to `True` before running the training function. To load the stored weights, set the variable `weights=/path/to/weights` in the training loop function.\n",
        "\n",
        "> Note: Weights saving can be enabled only for CAN and CAN improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJhYtj6cxlXd"
      },
      "source": [
        "## Notebook Initialization\n",
        "This section will include imports and installation of python packages, data related operations, model and optimizer definition and other useful procedures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T-5QL-Vya7K"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGzrhtq3vcEU",
        "outputId": "a1be4a56-0c5e-4fd1-8431-99348c9cfd1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJgrCCndwExs"
      },
      "source": [
        "### Preparation of the notebook\n",
        "Install wandb, [spherecluser](https://github.com/rfayat/spherecluster) (an implementation of the Spherical K-Means used to implement CAN) and a specific version of scikit-learn (required for Sphere Cluster to work)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWzwDL9JVA19"
      },
      "outputs": [],
      "source": [
        "!pip install wandb --quiet\n",
        "!pip install scikit-learn==0.24.2 --quiet # To use spherecluster https://stackoverflow.com/a/68182958/17566218\n",
        "                                          # this breaks the dependecy with yellowbrick but it is needed\n",
        "!pip install git+https://github.com/rfayat/spherecluster.git@scikit_update --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVCDbbe3-N-z"
      },
      "source": [
        "Import all required Python Modules, perform the login on wandb and define global constants.\n",
        "\n",
        "Moreover, checks the availability of \"cuda\" and set the variable `device` accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZCkDTZjUoFt"
      },
      "outputs": [],
      "source": [
        "#  imports\n",
        "import os\n",
        "import shutil\n",
        "import copy\n",
        "import wandb\n",
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataset import Subset\n",
        "from torch.utils.data.sampler import BatchSampler\n",
        "from sklearn.decomposition import KernelPCA\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from spherecluster.spherical_kmeans import SphericalKMeans\n",
        "\n",
        "# install wandb and login\n",
        "#%env WANDB_MODE=disabled\n",
        "wandb.login()\n",
        "\n",
        "# set device globally\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Selected Device: {device}\")\n",
        "\n",
        "# used in order to always have the same dataset\n",
        "generator = torch.Generator().manual_seed(0)\n",
        "\n",
        "# define constants\n",
        "NUM_CLASSES = 20 # required in the assignment\n",
        "BATCH_SIZE = 256 # empiric limit of Tesla K80 Colab GPU's\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "SPLIT_RATIO = 0.8\n",
        "BETA = 0.3 # scaling of CDD loss as described in CAN paper\n",
        "DEFAULT_EPOCHS = 10 # default number of epochs for traing\n",
        "DEFAULT_MOMENTUM = 0.9\n",
        "LR_SCHEDULER_A = 10 # default 'a' value for the sgd scheduler as described in can paper\n",
        "LR_SCHEDULER_B = 0.75 # default 'b' value for the sgd scheduler as described in can paper\n",
        "DEFAULT_OPTIM = 'adam' # default optimizer\n",
        "DEFAULT_NUM_WORKERS = 2 # empiric limit of colab as number of processes to use\n",
        "DEFAULT_D_0 = 0.05 # default minimum allowed distance from a sample \n",
        "                   # to its cluster centroid in order to be taken into consideration\n",
        "                   # for CDD loss. set as described in CAN paper\n",
        "DEFAULT_N_0 = 3 # default minimum number of samples (actually N_0+1) that satisfy the distance\n",
        "                # constraint (D_0) in order for a calss to be into consideration\n",
        "                # for CDD loss. set as described in CAN paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPAYFKSfyhFB"
      },
      "source": [
        "### Unzip full dataset in ```/content```\n",
        "By unzipping directly in the ```/content``` directory we do not have to remove the zipped file afterwards. The result is the full dataset downloaded and unzipped in the working directory.\n",
        "> Note: ```-qq``` inhibit the log."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wGdYt3RGxwLh"
      },
      "outputs": [],
      "source": [
        "# Quietly unzip the dataset\n",
        "!unzip -qq /content/drive/MyDrive/datasets/Adaptiope.zip "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5azE-Xi03EU"
      },
      "source": [
        "### Store a subset of Adaptiope\n",
        "Load the 20 selected classes for the assignment and store the subset of the Dataset.\n",
        "> Code adapted from the provided one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMXYTSjIyFqN"
      },
      "outputs": [],
      "source": [
        "classes = [\"backpack\", \"bookcase\", \"car jack\", \"comb\", \"crown\", \"file cabinet\", \"flat iron\", \"game controller\", \"glasses\",\n",
        "           \"helicopter\", \"ice skates\", \"letter tray\", \"monitor\", \"mug\", \"network switch\", \"over-ear headphones\", \"pen\",\n",
        "           \"purse\", \"stand mixer\", \"stroller\"]\n",
        "\n",
        "for d, td in zip([\"Adaptiope/product_images\", \"Adaptiope/real_life\"], [\"adaptiope_small/product_images\", \"adaptiope_small/real_life\"]):\n",
        "    for c in tqdm(classes):\n",
        "        c_path = os.path.join(d, c)\n",
        "        c_target = os.path.join(td, c)\n",
        "        shutil.copytree(c_path, c_target)\n",
        "\n",
        "shutil.rmtree('Adaptiope')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7dbjrlGA40O"
      },
      "source": [
        "### Load and split dataset\n",
        "Creates two dataset:\n",
        "1. Products;\n",
        "2. Real.\n",
        "\n",
        "Required for the UDA task.\n",
        "\n",
        "Moreover here we resize to images in order to match ResNet18 input dimensions and normalize according to ImageNet mean and std.\n",
        "As CAN authors did, we decided not to use data augmentation in order to have results which can be explained only through the architecture and training procedures we employed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0DXJriRzEf8"
      },
      "outputs": [],
      "source": [
        "# Create transformation sequence\n",
        "#   - Resize to match resnet input dimensions\n",
        "#   - Transform into a Tensor\n",
        "#   - Normalize with ImageNet mean and std.\n",
        "transformation_seq = [\n",
        "    transforms.Resize((224, 224)),           # Same input size of resnet\n",
        "    transforms.ToTensor(),                   # convert PIL to pytorch Tensor\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN,\n",
        "                         std=IMAGENET_STD)   # normalize with ImageNet mean and std\n",
        "]\n",
        "transformations = transforms.Compose(transformation_seq)\n",
        "\n",
        "# Load datasets and apply transformations\n",
        "products = torchvision.datasets.ImageFolder('/content/adaptiope_small/product_images', transformations)\n",
        "reals = torchvision.datasets.ImageFolder('/content/adaptiope_small/real_life', transformations)\n",
        "\n",
        "# Select 2 random images from the real and products datasets and show them\n",
        "idx = random.randint(0, len(products))\n",
        "f, axarr = plt.subplots(1, 2)\n",
        "\n",
        "# Transform to show unnormalized images\n",
        "invTransform = transforms.Compose([ transforms.Normalize(mean = [ 0., 0., 0. ],\n",
        "                                                     std = [1/item for item in IMAGENET_STD]),\n",
        "                                    transforms.Normalize(mean = [-item for item in IMAGENET_MEAN],\n",
        "                                                     std = [ 1., 1., 1. ]),\n",
        "])\n",
        "\n",
        "# Permute in order to shift channels in last dimension\n",
        "axarr[0].imshow(invTransform(products[idx][0]).permute(1, 2, 0))\n",
        "axarr[1].imshow(invTransform(reals[idx][0]).permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PwaV1fNvEC4"
      },
      "source": [
        "Split both dataset into train and test set. We used a **ratio** of 0.8 as requested by the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOwqPG_DA60E"
      },
      "outputs": [],
      "source": [
        "assert len(products) == len(reals), \"Products and reals are not the same length\"\n",
        "\n",
        "len_ds = len(products)  # Length of the dataset (products or reals)\n",
        "len_tr = int(SPLIT_RATIO * len_ds)  # compute train size\n",
        "len_ts = len_ds - len_tr  # compute test size\n",
        "\n",
        "# split product dataset\n",
        "train_products, test_products = torch.utils.data.random_split(products, [len_tr, len_ts], generator=generator)\n",
        "# split test dataset\n",
        "train_real, test_real = torch.utils.data.random_split(reals, [len_tr, len_ts], generator=generator)\n",
        "\n",
        "print(f\"Training Size: {len_tr} - Test Size: {len_ts}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwLaBa4EC1wN"
      },
      "source": [
        "### Create Dataloaders\n",
        "Create a dataloader for each domain and split. The number of workers is set to 2 as suggested by a `UserWarning`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4hFRcsKC1a1"
      },
      "outputs": [],
      "source": [
        "# Create a dataloader for each domain and split\n",
        "# num_workers set to 2 as suggested by UserWarning\n",
        "tr_dl_products = DataLoader(train_products, BATCH_SIZE, shuffle=True, num_workers=DEFAULT_NUM_WORKERS, pin_memory=True)\n",
        "ts_dl_products = DataLoader(test_products, BATCH_SIZE, shuffle=False, num_workers=DEFAULT_NUM_WORKERS, pin_memory=True)\n",
        "tr_dl_real = DataLoader(train_real, BATCH_SIZE, shuffle=True, num_workers=DEFAULT_NUM_WORKERS, pin_memory=True)\n",
        "ts_dl_real = DataLoader(test_real, BATCH_SIZE, shuffle=False, num_workers=DEFAULT_NUM_WORKERS, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM-lGm5vG-M2"
      },
      "source": [
        "### Netword definition\n",
        "Definition of a custom `nn.Module` in order to download the pretrained weights of the ResNet18 and to substitute the last linear layer to match the number of classes.\n",
        "\n",
        "We decided to aggregate everything into a single class so that the instantiation was cleaner.\n",
        "\n",
        "In this cell we also defined a snippet that allows us to hook the activations of the Network. Useful to compute the CDD loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvltpQC2GH5_"
      },
      "outputs": [],
      "source": [
        "# extract activations from network\n",
        "# this snippet is taken from the link below\n",
        "# https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/6\n",
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, out_dim):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.num_classes = out_dim\n",
        "        # Get resnet18 pretrained\n",
        "        self.backbone = models.resnet18(pretrained=True)\n",
        "        # get number of output feature from the penultimate layer\n",
        "        num_ftrs = self.backbone.fc.in_features\n",
        "        # replace last Linear layer:\n",
        "        #   - input number of feature of previous layer\n",
        "        #   - output dimension equal to number of classes taken \n",
        "        #     into consideration for the assignment\n",
        "        self.backbone.fc = nn.Linear(num_ftrs, self.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I3OqRzY72s2"
      },
      "source": [
        "### Define cost function and optimizer\n",
        "Define functions to get:\n",
        "* The **crossentropy** loss function;\n",
        "* An optimizer (adam or sgd) with lower learning rate for pre-trained layers of the network:\n",
        "    * **Adam**: Return Adam optimizer;\n",
        "    * **SGD**: Return Stochastic Gradient Descent optimizer.\n",
        "* A **scheduler**. Used in the vanilla implementation of CAN, as described in the paper: $$\\eta_p = \\frac{\\eta_0}{(1 + ap)^b}$$ where $p$ linearly increases from 0 to 1 and represent the progress throughout the epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5yTyHS27yMY"
      },
      "outputs": [],
      "source": [
        "def get_ce():\n",
        "    return nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def get_optimizer(model, lr, optim:str=DEFAULT_OPTIM):\n",
        "    '''\n",
        "    optim: either 'adam' or 'sgd'\n",
        "    '''\n",
        "    final_layer_weights = []\n",
        "    rest_of_the_net_weights = []\n",
        "\n",
        "    # iterate through the layers of the network\n",
        "    for name, param in model.named_parameters():\n",
        "        if name.startswith('bacbone.fc'):\n",
        "            final_layer_weights.append(param)\n",
        "        else:\n",
        "            rest_of_the_net_weights.append(param)\n",
        "\n",
        "    # assign the distinct learning rates to each group of parameters\n",
        "    lr_specs = [\n",
        "            {'params': rest_of_the_net_weights},\n",
        "            {'params': final_layer_weights, 'lr': lr}\n",
        "        ]\n",
        "    if (optim == 'adam'):\n",
        "        optimizer = torch.optim.Adam(lr_specs, lr=lr/10)\n",
        "    elif (optim == 'sgd'):\n",
        "        optimizer = torch.optim.SGD(lr_specs, lr=lr/10, momentum=DEFAULT_MOMENTUM)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def get_lr_scheduler(optimizer, tot_epochs):\n",
        "    a = LR_SCHEDULER_A\n",
        "    b = LR_SCHEDULER_B\n",
        "    coeff_func = lambda epoch: 1 / ((1 + a * (epoch / tot_epochs))**b)\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=coeff_func)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqnbJO6P9D1Z"
      },
      "source": [
        "### Performance visualization\n",
        "Define a function in order to visualize the performance of the model. Prints the classification report, confusion matrix plot and T-SNE plot to visualize samples in a 2D space: similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFift2Y99B6w"
      },
      "outputs": [],
      "source": [
        "def visualize(model, dataloader, wandb_run):\n",
        "    '''\n",
        "    Function to log additional metrics to wandb such as confusion matrix, \n",
        "    T-SNE scatter plot, images with wrong predictions.\n",
        "    No training is done here.\n",
        "    '''\n",
        "    model.backbone.avgpool.register_forward_hook(get_activation('phi_1'))\n",
        "\n",
        "    phi_list = []\n",
        "    y_pred_list = []\n",
        "    y_true_list = []\n",
        "    incorrect_examples_list = []\n",
        "    incorrect_preds_list = []\n",
        "    incorrect_probs_list = []\n",
        "    incorrect_y_true_list = []\n",
        "    with torch.no_grad():\n",
        "        # Forward pass to compute predictions \n",
        "        for _, (inputs, labels) in enumerate(dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            preds = outputs.max(dim=1)[1]\n",
        "            probs = F.softmax(outputs).max(dim=1)[0]\n",
        "\n",
        "            phi_list.append(activation['phi_1'].squeeze())\n",
        "            y_pred_list.append(preds)\n",
        "            y_true_list.append(labels)\n",
        "\n",
        "            # get a mask with wrong images indexes.\n",
        "            idxs_mask = (preds != labels).view(-1)\n",
        "            incorrect_examples_list.append(inputs[idxs_mask])\n",
        "            incorrect_preds_list.append(preds[idxs_mask])\n",
        "            incorrect_y_true_list.append(labels[idxs_mask])\n",
        "            incorrect_probs_list.append(probs[idxs_mask])\n",
        "\n",
        "    # Concat lists into tensors\n",
        "    incorrect_examples = torch.cat(incorrect_examples_list)\n",
        "    incorrect_preds = torch.cat(incorrect_preds_list)\n",
        "    incorrect_y_true = torch.cat(incorrect_y_true_list)\n",
        "    incorrect_probs = torch.cat(incorrect_probs_list)\n",
        "    y_preds = torch.cat(y_pred_list).cpu().numpy()\n",
        "    y_trues = torch.cat(y_true_list).cpu().numpy()\n",
        "    phis = torch.cat(phi_list).cpu().numpy()\n",
        "    \n",
        "    # Take five random wrong predicted images and plot them on wandb.\n",
        "    rand_indexes = np.random.randint(low=0, high=incorrect_examples.shape[0], size=5)\n",
        "    wrong_preds_table = wandb.Table(columns=['Image', 'Prediction', 'Ground Truth', 'Probability'])\n",
        "    for rnd_idx in rand_indexes:\n",
        "        wrong_preds_table.add_data(\n",
        "            wandb.Image(incorrect_examples[rnd_idx]), \n",
        "            classes[incorrect_preds[rnd_idx]], \n",
        "            classes[incorrect_y_true[rnd_idx]], \n",
        "            incorrect_probs[rnd_idx]\n",
        "            )\n",
        "    wandb.log({\"test/wrong_predictions\": wrong_preds_table}, commit=False)\n",
        "\n",
        "    # Print classification report\n",
        "    print(classification_report(y_trues, y_preds, target_names=classes))\n",
        "\n",
        "    # Build and log confusion matrix to wandb\n",
        "    cf_matrix = confusion_matrix(y_trues, y_preds, normalize=\"true\")\n",
        "    df_cm = pd.DataFrame(cf_matrix, index=classes, columns=classes)\n",
        "    plt.figure(0, figsize = (12,7))\n",
        "    sns.heatmap(df_cm, annot=True)\n",
        "    cm_plot = plt.figure(0)\n",
        "    plt.savefig(f\"cm-{wandb_run.project}-{wandb_run.name}.png\")\n",
        "    wandb.log({\"test/confusion_matrix_img\": wandb.Image(cm_plot)}, commit=False)\n",
        "\n",
        "    # Apply T-SNE on last avgpool layer activations to visualize data separation\n",
        "    # Adapted from https://www.datatechnotes.com/2020/11/tsne-visualization-example-in-python.html\n",
        "    perplexity = 5\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=123)\n",
        "    phis_reduced = tsne.fit_transform(phis) \n",
        "    df_tsne = pd.DataFrame()\n",
        "    df_tsne[\"y\"] = y_trues\n",
        "    df_tsne[\"comp-1\"] = phis_reduced[:,0]\n",
        "    df_tsne[\"comp-2\"] = phis_reduced[:,1]\n",
        "\n",
        "    plt.figure(1, dpi=100)\n",
        "    sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df_tsne.y.tolist(),\n",
        "                    palette=sns.color_palette(\"hls\", NUM_CLASSES),\n",
        "                    data=df_tsne, legend=False).set(title=\"Last avgpool layer activations T-SNE projection\")\n",
        "    plt.tick_params(left=False, right=False, labelleft=False, \n",
        "                    labelbottom=False, bottom=False)\n",
        "    tsne_plot = plt.figure(1, dpi=100)\n",
        "    plt.savefig(f\"tsne-{perplexity}-{wandb_run.project}-{wandb_run.name}.png\")\n",
        "    wandb.log({\"test/tsne_visualization\": wandb.Image(tsne_plot)}, commit=False)\n",
        "    cf_matrix_wandb = wandb.sklearn.plot_confusion_matrix(y_trues, y_preds, classes, normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LO9LfLkHDuv"
      },
      "source": [
        "## Baseline Approach\n",
        "Here we present the first attempt to solve the problem. As just mentioned above, we fine-tune a ResNet18 on the source training set and we test it on the target test set. In this case we **do not** adopt any Domain Adaptation technique, we just evaluate the performace of ResNet in the case of Domain Shift.\n",
        "\n",
        "We can also appreciate the upper bound accuracy in the supervised scenario for both domains, computed as a standard classification task using training and test set from a single domain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHY-T_ppXqAe"
      },
      "source": [
        "### Define baseline training step and test step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSyMcdmXMVqX"
      },
      "outputs": [],
      "source": [
        "def training_step_baseline(model, data_loader, optimizer, cost_function, device, epoch=0):\n",
        "    n_samples = 0\n",
        "    cumulative_loss = 0.\n",
        "    cumulative_accuracy = 0.\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(tqdm_notebook(data_loader, desc=\"Training Step\", leave=False)):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = cost_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # add batch size\n",
        "        n_samples += inputs.shape[0]\n",
        "        # cumulative loss\n",
        "        cumulative_loss += loss.item()\n",
        "        # return predicted labels\n",
        "        max_prob, predicted = outputs.max(dim=1)\n",
        "        # cumulative accuracy\n",
        "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "    # avg loss and accuracy\n",
        "    loss = cumulative_loss / n_samples\n",
        "    acc = cumulative_accuracy / n_samples\n",
        "\n",
        "    metrics = {\n",
        "        \"train/train_loss\": loss,\n",
        "        \"train/train_acc\": acc\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def test_step_baseline(model, data_loader, cost_function, device, epoch=0):\n",
        "    n_samples = 0\n",
        "    cumulative_loss = 0.\n",
        "    cumulative_accuracy = 0.\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(tqdm_notebook(data_loader, desc=\"Test Step\", leave=False)):\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = cost_function(outputs, targets)\n",
        "\n",
        "            # add batch size\n",
        "            n_samples += inputs.shape[0]\n",
        "            # cumulative loss\n",
        "            cumulative_loss += loss.item()\n",
        "            # return predicted labels\n",
        "            max_prob, predicted = outputs.max(dim=1)\n",
        "\n",
        "            # cumulative accuracy\n",
        "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "    \n",
        "    # avg loss and accuracy\n",
        "    loss = cumulative_loss / n_samples\n",
        "    acc = cumulative_accuracy / n_samples\n",
        "\n",
        "    metrics = {\n",
        "        \"test/test_loss\": loss,\n",
        "        \"test/test_acc\": acc\n",
        "    }\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7aa4s5kX-ZQ"
      },
      "source": [
        "### Train baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1jhYzdQVJlC"
      },
      "outputs": [],
      "source": [
        "def training_loop_baseline(tr_dl, ts_dl, device, wandb_run):\n",
        "    print(wandb_run.name)\n",
        "\n",
        "    model = ResNet18(NUM_CLASSES).to(device)\n",
        "\n",
        "    optimizer = get_optimizer(model, lr=wandb.config['lr'], optim=wandb.config[\"optimizer\"])\n",
        "    cost_fn = get_ce()\n",
        "    \n",
        "    best_loss = 0.\n",
        "    best_acc = 0.\n",
        "\n",
        "    print(\"Start training\")\n",
        "    for e in tqdm_notebook(range(wandb_run.config['epochs']), desc=\"Training Loop\"):\n",
        "        train_metrics = training_step_baseline(model, tr_dl, optimizer, cost_fn, device, epoch=e)\n",
        "        test_metrics = test_step_baseline(model, ts_dl, cost_fn, device, epoch=e)\n",
        "\n",
        "        wandb.log({**train_metrics, **test_metrics})\n",
        "\n",
        "        train_loss = train_metrics['train/train_loss']\n",
        "        train_acc = train_metrics['train/train_acc']\n",
        "        \n",
        "        test_loss = test_metrics['test/test_loss']\n",
        "        test_acc = test_metrics['test/test_acc']\n",
        "\n",
        "        if best_acc < test_acc or e == 0:\n",
        "            best_acc = test_acc\n",
        "            best_loss = test_loss\n",
        "            best_model = copy.deepcopy(model)\n",
        "        \n",
        "        print('\\n Epoch: {:d}'.format(e + 1))\n",
        "        print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_acc))\n",
        "        print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_acc))\n",
        "        print('-----------------------------------------------------')\n",
        "\n",
        "    visualize(best_model, ts_dl, wandb_run)\n",
        "    wandb.summary[\"test_best_loss\"] = best_loss\n",
        "    wandb.summary[\"test_best_accuracy\"] = best_acc\n",
        "    wandb.finish()\n",
        "    print('\\t BEST Test loss {:.5f}, Test accuracy {:.2f}'.format(best_loss, best_acc))\n",
        "    \n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB-EvSED0xx0"
      },
      "source": [
        "Train baseline model on Products, test on Real World."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnLkiecuZ512"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    project=\"DL2022_229356_229298\",\n",
        "    entity=\"229356_229298\",\n",
        "    name='P_to_R_baseline',\n",
        "    config={\n",
        "        \"model\": \"ResNet18\",\n",
        "        \"trained-on\": \"Source only\",\n",
        "        \"epochs\": 30,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"optimizer\": \"sgd\",\n",
        "        \"lr\": 1e-2,\n",
        "        \"loss\": \"CrossEntropyLoss\"\n",
        "    }\n",
        ")\n",
        "\n",
        "best_P_R = training_loop_baseline(tr_dl_products, ts_dl_real, device, run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1_b0BnqwXWh"
      },
      "source": [
        "**Best test accuracy $P \\rightarrow R:$ 0.74**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh4kTLqc01l2"
      },
      "source": [
        "Train baseline model on Real World, test on Products."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvxWwMyNewEB"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    project=\"DL2022_229356_229298\",\n",
        "    entity=\"229356_229298\",\n",
        "    name='R_to_P_baseline',\n",
        "    config={\n",
        "        \"model\": \"ResNet18\",\n",
        "        \"trained-on\": \"Source only\",\n",
        "        \"epochs\": 30,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"optimizer\": \"sgd\",\n",
        "        \"lr\": 1e-2,\n",
        "        \"loss\": \"CrossEntropyLoss\"\n",
        "    }\n",
        ")\n",
        "\n",
        "best_R_P = training_loop_baseline(tr_dl_real, ts_dl_products, device, run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC2v4yoYwFD_"
      },
      "source": [
        "**Best test accuracy $R \\rightarrow P:$ 0.93**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWrWfdtBgQKj"
      },
      "source": [
        "Computer Upper Bound accuracy on Products."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCEbVD_DFeJl"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    project=\"DL2022_229356_229298\",\n",
        "    entity=\"229356_229298\",\n",
        "    name='Upper_Bound_P',\n",
        "    config={\n",
        "        \"model\": \"ResNet18\",\n",
        "        \"trained-on\": \"Source only: P\",\n",
        "        \"epochs\": 30,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"optimizer\": \"sgd\",\n",
        "        \"lr\": 1e-2,\n",
        "        \"loss\": \"CrossEntropyLoss\"\n",
        "    }\n",
        ")\n",
        "\n",
        "model_upper_bound_P = training_loop_baseline(tr_dl_products, ts_dl_products, device, run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5UXL31agaep"
      },
      "source": [
        "precision    | recall |  f1-score |  support\n",
        "---|---|---|---\n",
        "backpack            | 0.97   |   1.00  |   0.98   |    29\n",
        "bookcase            | 0.91   |   1.00  |   0.95   |    21\n",
        "car jack            | 0.88   |   0.88  |   0.88   |    17\n",
        "comb                | 0.86   |   0.95  |   0.90   |    19\n",
        "crown               | 1.00   |   1.00  |   1.00   |    20\n",
        "file cabinet        | 0.94   |   0.89  |   0.91   |    18\n",
        "flat iron           | 0.76   |   1.00  |   0.86   |    16\n",
        "game controller     | 0.95   |   0.83  |   0.89   |    24\n",
        "glasses             | 1.00   |   1.00  |   1.00   |    19\n",
        "helicopter          | 0.89   |   1.00  |   0.94   |    17\n",
        "ice skates          | 0.94   |   0.89  |   0.92   |    19\n",
        "letter tray         | 1.00   |   1.00  |   1.00   |    16\n",
        "monitor             | 1.00   |   0.90  |   0.95   |    20\n",
        "mug                 | 1.00   |   1.00  |   1.00   |    17\n",
        "network switch      | 1.00   |   1.00  |   1.00   |    24\n",
        "over-ear headphones | 1.00   |   1.00  |   1.00   |    15\n",
        "pen                 | 0.92   |   0.83  |   0.87   |    29\n",
        "purse               | 1.00   |   0.90  |   0.95   |    21\n",
        "stand mixer         | 1.00   |   1.00  |   1.00   |    19\n",
        "stroller            | 1.00   |   1.00  |   1.00   |    20\n",
        " |||\n",
        "accuracy            |        |         |   0.95   |   400\n",
        "macro avg           | 0.95   |   0.95  |   0.95   |   400\n",
        "weighted avg        | 0.95   |   0.95  |   0.95   |   400"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6pL4wIRgS-3"
      },
      "source": [
        "**Upper Bound Products Acc: 0.95**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzoVDeuCgLva"
      },
      "source": [
        "Computer Upper Bound accuracy on Real."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVnrePuz0ffK"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    project=\"DL2022_229356_229298\",\n",
        "    entity=\"229356_229298\",\n",
        "    name='Upper_Bound_R',\n",
        "    config={\n",
        "        \"model\": \"ResNet18\",\n",
        "        \"trained-on\": \"Source only: R\",\n",
        "        \"epochs\": 30,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"optimizer\": \"sgd\",\n",
        "        \"lr\": 1e-2,\n",
        "        \"loss\": \"CrossEntropyLoss\"\n",
        "    }\n",
        ")\n",
        "\n",
        "model_upper_bound_R = training_loop_baseline(tr_dl_real, ts_dl_real, device, run)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQZnSJb4qCn2"
      },
      "source": [
        "class | precision   | recall | f1-score |  support\n",
        "---|---|---|---|---\n",
        "backpack            |   0.77    |  1.00  |    0.87  |      17\n",
        "bookcase            |   0.94    |  0.80  |    0.86  |      20\n",
        "car jack            |   0.83    |  1.00  |    0.91  |      15\n",
        "comb                |   0.83    |  0.91  |    0.87  |      22\n",
        "crown.              |   1.00    |  1.00  |    1.00  |      21\n",
        "file cabinet.       |   0.83    |  0.91  |    0.87  |      22\n",
        "flat iron           |   0.94    |  0.94  |    0.94  |      16\n",
        "game controller     |   0.95    |  0.90  |    0.93  |      21\n",
        "glasses.            |   0.95    |  1.00  |    0.97  |      19\n",
        "helicopter          |   1.00    |  0.95  |    0.97  |      19\n",
        "ice skates          |   0.94    |  0.81  |    0.87  |      21\n",
        "letter tray         |   0.92    |  0.85  |    0.88  |      27\n",
        "monitor             |   0.95    |  0.90  |    0.92  |      20\n",
        "mug                 |   0.88    |  0.92  |    0.90  |      24\n",
        "network switch      |   0.81    |  1.00  |    0.89  |      17\n",
        "over-ear headphones |   1.00    |  1.00  |    1.00  |      17\n",
        "pen                 |   0.94    |  0.94  |    0.94  |      17\n",
        "purse               |   1.00    |  0.74  |    0.85  |      23\n",
        "stand mixer         |   0.95    |  0.95  |    0.95  |      21\n",
        "stroller            |   1.00    |  0.95  |    0.98  |      21\n",
        "||||\n",
        "accuracy            |           |        |    0.92  |     400\n",
        "macro avg           |   0.92    |  0.92  |    0.92  |     400\n",
        "weighted avg        |   0.92    |  0.92  |    0.92  |     400"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hewZJHcOgYPI"
      },
      "source": [
        "**Upper Bound RealWorld Acc: 0.92**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "limppqcxuE-s"
      },
      "source": [
        "### Observations\n",
        "Before going into the observations, we must recall that each Network is trained on the source training set and tested on the target test set.\n",
        "\n",
        "We can observe how test accuracies are quite high just by using a pretrained ResNet18. This is thanks to the pretraining of the network on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) dataset which allows the network to learn general features that transfer well on Adaptiope dataset.\n",
        "\n",
        "It might seem that the training suffers from overfitting but we are saving the best model (i.e. the model with highest test accuracy) and therefore applying some form of early stopping to cope with possibile overfitting. \n",
        "In the Tables below we have a summary of the accuracies we get. \n",
        "\n",
        "**Product to Real World** - \n",
        "Of the two domain adaptation tasks, $P → R$ is the most difficult one. In fact the overall accuracy obtained is lower and several classes (like \"glasses\") have low classwise F1-score. Moreover we can observe that the confusion matrix is very noisy and that the t-SNE representation is not very accurate. As a reason for this behaviour, our hypothesis is that the Real World domain presents objects in different orientations and light conditions (often poor) and mixed up with other objects. Furthermore the classes with lower accuracy are the ones where objects are very similar, for example \"purse\" and \"backpack\". The Product domain instead presents samples in which the objects are isolated and with good illumination. Therefore the model trained on the Product domain does not have a good transfer on Real World domain.\n",
        "\n",
        "       \n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1frS4h0qSPAqZ2Q2gcqT9Zo8wsjJILJCw\" width=\"500px\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Eo6D3KUQEZBogOBJ1KywNs5NJ-cmZTp8\" width=\"500px\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1fcKDWMseKCiGcUeZb4Pr14ECZPGRzWUD\" height=\"400px\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1I24sggX5SDkQD7FjbmEvErCwm9aLLU0E\" height=\"300px\">\n",
        "\n",
        "class | precision | recall | f1-score  | support\n",
        "------|-----------|--------|-----------|--------\n",
        "backpack       |0.44      |0.94      |0.60        |17\n",
        "bookcase       |0.81      |0.65      |0.72        |20\n",
        "car jack       |0.80      |0.80      |0.80        |15\n",
        "comb       |0.52      |0.77      |0.62        |22\n",
        "crown       |0.91      |0.95      |0.93        |21\n",
        "file cabinet       |0.56      |0.64      |0.60        |22\n",
        "flat iron       |0.93      |0.81      |0.87        |16\n",
        "game controller       |1.00      |0.76      |0.86        |21\n",
        "glasses       |0.90      |0.47      |0.62        |19\n",
        "helicopter       |0.78      |0.95      |0.86        |19\n",
        "ice skates       |0.88      |0.67      |0.76        |21\n",
        "letter tray       |0.73      |0.70      |0.72        |27\n",
        "monitor       |0.79      |0.75      |0.77        |20\n",
        "mug       |1.00      |0.88      |0.93        |24\n",
        "network switch       |0.82      |0.53      |0.64        |17\n",
        "over-ear headphones       |0.84      |0.94      |0.89        |17\n",
        "pen       |0.86      |0.71      |0.77        |17\n",
        "purse       |0.50      |0.48      |0.49        |23\n",
        "stand mixer       |0.76      |0.90      |0.83        |21\n",
        "stroller       |0.82      |0.67      |0.74        |21\n",
        "||||\n",
        "accuracy     |          |          |  0.74    |   400\n",
        "macro avg     |  0.78    |  0.75    |  0.75    |   400\n",
        "weighted avg     |  0.78    |  0.74    |  0.75    |   400\n",
        "\n",
        "**Real World to Product** - \n",
        "In the case of $R → P$ the baseline performances are higher. The main reason could be training on the Real World dataset allows to generalize better, getting as a consequence a higher accuracy on the target test set. As we can see, both the confusion matrix and the t-SNE representation are accurate.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1KoZ19gAQ7IrimxR5zj9QqS9mWHO5eYx8\" width=\"500px\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1azMQXE7HW2UOom3RsRu9eqJjLITi44gV\" width=\"500px\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=16BmAsC_tE-pvEcHx6IMlOZnPUlo1uUXT\" height=\"400px\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Eha_2alrv81awDCwm9FEkh6BsaZ9f9Ck\" height=\"300px\">\n",
        "\n",
        "class | precision  |  recall | f1-score |  support\n",
        "------|------------|---------|----------|----------\n",
        "backpack       |0.96      |0.90      |0.93        |29\n",
        "bookcase       |0.86      |0.86      |0.86        |21\n",
        "car jack       |0.93      |0.82      |0.87        |17\n",
        "comb       |0.83      |1.00      |0.90        |19\n",
        "crown       |1.00      |1.00      |1.00        |20\n",
        "file cabinet       |0.88      |0.78      |0.82        |18\n",
        "flat iron       |0.88      |0.94      |0.91        |16\n",
        "game controller       |0.95      |0.88      |0.91        |24\n",
        "glasses       |0.95      |1.00      |0.97        |19\n",
        "helicopter       |0.89      |1.00      |0.94        |17\n",
        "ice skates       |1.00      |0.95      |0.97        |19\n",
        "letter tray       |0.89      |1.00      |0.94        |16\n",
        "monitor       |1.00      |0.90      |0.95        |20\n",
        "mug       |1.00      |1.00      |1.00        |17\n",
        "network switch       |1.00      |0.96      |0.98        |24\n",
        "over-ear headphones       |0.79      |1.00      |0.88        |15\n",
        "pen       |0.89      |0.83      |0.86        |29\n",
        "purse       |0.86      |0.86      |0.86        |21\n",
        "stand mixer       |1.00      |0.95      |0.97        |19\n",
        "stroller       |0.95      |1.00      |0.98        |20\n",
        "||||\n",
        "accuracy   |            |          |  0.93    |  400\n",
        "macro avg   |    0.93    |  0.93    |  0.93    |  400\n",
        "weighted avg   |    0.93    |  0.93    |  0.92    |  400\n",
        "\n",
        "**Upper bounds** -\n",
        "The upper bound on the accuracy for the Products dataset is 0.95 while in the Real World one is 0.92. We need to take into account these numbers while conducting performance evaluation of CAN domain adaptation method and our improvements on it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1biDm0iveg8z"
      },
      "source": [
        "## CAN Implementation\n",
        "As mentioned above, in order to increase the performance from the baseline results we decided to implement the [Contrastive Adaptation Network](https://arxiv.org/abs/1901.00976) by Kang et. al. \n",
        "\n",
        "CAN is a discrepancy based domain adaptation method which aims at minimizing the discrepancy between the source and target domains, through statistical domain alignment.\n",
        "\n",
        "Previous discrepancy based methods measured domain discrepancy by Maximum Mean Discrepancy (MMD) and Joint MMD, obtaining state of the art results on several UDA benchmarks. Despite the success of those methods, they all have a common problem which is the fact that the domain discrepancy is measured at the domain level, neglecting the class from which the samples are drawn. \n",
        "\n",
        "By performing a class-agnostic domain alignment, the MMD and JMMD can be minimized even when target-domain samples are misaligned w.r.t. source-domain samples of a different class. Thus the decision boundary may generalize poorly for the target domain. \n",
        "\n",
        "The authors of CAN, instead, propose a discrepancy measure which explicitly takes into consideration the class information. For this purpose labels for both domains are needed, therefore they use clustering in order to infer pseudolabels for the target set. \n",
        "That given, they propose the Contrastive Domain Discrepancy (CDD) which is established on the difference between conditional data distribution across domains. Using this approach, authors were able to obtain SOTA results in Unsupervised Domain Adaptation. \n",
        "\n",
        "![Architecture](https://drive.google.com/uc?export=view&id=18zSj57F5lBfVRjqjtZ9saJXeD3RUI4iP)\n",
        "\n",
        "Training process of CAN. To minimize CDD, authors perform alternative optimization between updating the target label hypothesis, through clustering, and adapting feature representations through bach-propagation.\n",
        "\n",
        "Each part of the algorithm will be explained in the following sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGDceleiek2Z"
      },
      "source": [
        "### CDD Loss\n",
        "The Contrastive Domain Discrepancy Loss is computed as follows:\n",
        "$$\n",
        "\\hat{D}^{cdd}_L = \\sum_{l=1}^{L} \\hat{D}_l^{cdd}\n",
        "$$\n",
        "where $l$ is the index of a Fully Connected Layer. It is known that CNNs are able to learn more transferable features than shallow models. However, the discrepancy still exists for domain-specific layers. In other words, convolutional layers extract general and more transferable features while FC layers exhibit abstract and domain specific features. Thus, they must be adapted and to do so we must compute $\\hat{D}^{cdd}$ to all fully connected layers of the network. In our case 2 layers are involved, the output of the global average pooling and the output layer.\n",
        "\n",
        "Each $\\hat{D}^{cdd}_l$ measures the difference between the intra- and inter-class domain discrepancy of the given layer, which will be optimized in opposite direction. As the training proceeds the intra-class domain discrepancy becomes smaller while the inter-class domain discrepancy becomes larger, so that the hard (ambiguous) classes are able to be taken into account. It is worth noting that, due to this property, is very likely that this value will be negative.\n",
        "$$\n",
        "\\hat{D}^{cdd}_l = \\underset{intra}{\\underbrace{\\frac{1}{M} \\sum_{c=1}^{M} \\hat{D}^{cc}(\\hat{y}_{1:n_t}^{t}, \\phi_l)}} - \\underset{inter}{\\underbrace{\\frac{1}{M(M-1)} \\sum_{c=1}^{M} \\sum_{c'=1\\\\c'\\neq c}^{M} \\hat{D}^{cc'} (\\hat{y}_{1:n_t}^{t}, \\phi_l)}}\n",
        "$$\n",
        "where $M$ is the number of classes, $\\phi_l$ is the output of the layer $l \\in L$ taken into consideration for computing $\\hat{D}^{cdd}_l$ and $\\hat{y}_{1:n_t}^{t}$ is the abbreviation of $\\hat{y}_1^t, ..., \\hat{y}_{n_t}^t$ which are the estimated classes for the target dataset ($n_t$ number of target examples).\n",
        "\n",
        "$\\hat{D}^{c_1 c_2}$ is then defined as follows:\n",
        "$$\n",
        "\\hat{D}^{c_1 c_2}(\\hat{y}_1^t, ..., \\hat{y}_{n_t}^t, \\phi) = e_1 + e_2 - 2e_3\n",
        "$$\n",
        "where:\n",
        "$$\n",
        "e_1 = \\sum_{i=1}^{n_s} \\sum_{j=1}^{n_s} \\frac{\\mu_{c_1 c_1}(y_i^s, y_j^s)k(\\phi(x_i^s), \\phi(x_j^s))}{\\sum_{i=1}^{n_s} \\sum_{j=1}^{n_s} \\mu_{c_1 c_1}(y_i^s, y_j^s)} \\\\\n",
        "e_2 = \\sum_{i=1}^{n_t} \\sum_{j=1}^{n_t} \\frac{\\mu_{c_2 c_2}(\\hat{y}_i^t, \\hat{y}_j^t)k(\\phi(x_i^t), \\phi(x_j^t))}{\\sum_{i=1}^{n_t} \\sum_{j=1}^{n_t} \\mu_{c_2 c_2}(\\hat{y}_i^t, \\hat{y}_j^t)} \\\\\n",
        "e_3 = \\sum_{i=1}^{n_s} \\sum_{j=1}^{n_t} \\frac{\\mu_{c_1 c_2}(y_i^s, \\hat{y}_j^t)k(\\phi(x_i^s), \\phi(x_j^t))}{\\sum_{i=1}^{n_s} \\sum_{j=1}^{n_t} \\mu_{c_1 c_2}(y_i^s, \\hat{y}_j^t)}\n",
        "$$\n",
        "\n",
        "$\\mu_{c c'}$ acts like a filter that selects only the examples from class $c$ and class $c'$:\n",
        "$$\n",
        "\\mu_{c c'}(y, y') = \\begin{cases}\n",
        "                        1 & \\text{ if } y=c, y'=c' \\\\ \n",
        "                        0 & \\text{ otherwise } \n",
        "                    \\end{cases}\n",
        "$$\n",
        "Clearly, to compute the mask $\\mu_{c_2 c_2}(\\hat{y}_i^t, \\hat{y}_j^t)$ and $\\mu_{c_1 c_2}(y_i^s, \\hat{y}_j^t)$ we need to estimate target labels. In Clustering section the approach used by the authors will be explained.\n",
        "\n",
        "$k$ is the kernel function that is used to compute the similarity between two activations.\n",
        "\n",
        "$\\hat{D}^{c_1 c_2}$ defines two kinds of class-aware domain discrepancies:\n",
        "1. when $c_1 = c_2 = c$, it measures the intra-class domain discrepancy;\n",
        "2. when $c_1 \\neq c_2$, it measures the inter-class domain discrepancy.\n",
        "\n",
        "The resulting $\\hat{D}^{cdd}$ will then be multiplied by a rescaling factor $\\beta$ and then added to the CrossEntropy Loss function computed on the source domain. The overall objective function is:\n",
        "$$\n",
        "\\underset{\\theta}{min}\\,l = l^{ce} + \\beta \\hat{D}_L^{cdd}\n",
        "$$\n",
        "\n",
        "> Note that also source examples are used in the computation of CDD but authors did not include them in functions' signature ($n_s$ number of source examples).\n",
        "\n",
        "![Discrepancy](https://drive.google.com/uc?export=view&id=19vqJ48RKssJLJIEIE_Iio4uWK8sXgx8-)\n",
        "\n",
        "Comparison between no adaptation, other domain-discrepancy minimization methods, and CAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsrn55XSUoF7"
      },
      "outputs": [],
      "source": [
        "def rbf_kernel(X, Y=None, gamma=None):\n",
        "    \"\"\"\n",
        "    Based on sklearn.metrics.pairwise.rbf_kernel implementation.\n",
        "    Compute the rbf (gaussian) kernel between X and Y:\n",
        "        K(x, y) = exp(-gamma ||x-y||^2)\n",
        "    for each pair of rows x in X and y in Y.\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : Tensor of shape (n_samples_X, n_features)\n",
        "    Y : Tensor of shape (n_samples_Y, n_features), default=None\n",
        "        If `None`, uses `Y=X`.\n",
        "    gamma : float, default=None\n",
        "        If None, defaults to 1.0 / n_features.\n",
        "    Returns\n",
        "    -------\n",
        "    kernel_matrix : Tensor of shape (n_samples_X, n_samples_Y)\n",
        "    \"\"\"\n",
        "    if Y is None:\n",
        "        Y = torch.clone(X)\n",
        "\n",
        "    if gamma is None:\n",
        "        gamma = 1.0 / X.shape[1]\n",
        "\n",
        "    K = torch.cdist(X, Y, compute_mode=\"use_mm_for_euclid_dist\").square()\n",
        "    K = torch.mul(K, -gamma)\n",
        "    return torch.exp(K)  # exponentiate K in-place"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpslO8TsUoF7"
      },
      "outputs": [],
      "source": [
        "def CDD_loss(source_phis, target_phis, source_labels, target_labels, classes, beta):\n",
        "    ''' \n",
        "    Compute the Contrastive Domain Discrepancy Loss between source \n",
        "    and target activations given source labels and target pseudo-labels,\n",
        "    for specified classes.\n",
        "    Parameters\n",
        "    ----------\n",
        "    source_phis : list containing each layer's source batched activations\n",
        "                  activation is a Tensor of shape (batch_size, num_features)\n",
        "    target_phis : list containing each layer's target batched activations\n",
        "                  activation is a Tensor of shape (batch_size, num_features)\n",
        "    source_labels : Tensor of shape (batch_size)\n",
        "    target_labels : Tensor of shape (batch_size)\n",
        "    classes : set of classes (indexes) on which the CDD is computed.\n",
        "    '''\n",
        "    cdd_loss = torch.tensor(0., device=device)\n",
        "    for phi_src, phi_tgt in zip(source_phis, target_phis):\n",
        "        cdd_loss = torch.add(cdd_loss, D_cdd(phi_src, source_labels, phi_tgt, target_labels, device, classes))\n",
        "    return torch.mul(cdd_loss, beta)\n",
        "\n",
        "\n",
        "def D_cdd(phi_source, y_source, phi_target, y_target, device, classes):\n",
        "    '''\n",
        "    Compute CDD Loss for a single source/target activation pair.\n",
        "    '''\n",
        "    num_classes = len(classes)\n",
        "\n",
        "    # intra_sum is the cumulative intra-class domain discrepancy. By minimizing it we try to align the domains of the same class\n",
        "    intra_sum = torch.tensor(0., device=device)\n",
        "    # On the other hand, inter_sum is the comulative intra-class domain discrepancy. This one is maximized in order to separate the distributions of two diffrent classes\n",
        "    inter_sum = torch.tensor(0., device=device)\n",
        "\n",
        "    # compute the cumulative intra-class domain discrepancy\n",
        "    for c in classes:\n",
        "        intra_sum = torch.add(intra_sum, domain_discrepancy(c, c, phi_source, y_source, phi_target, y_target, device))\n",
        "\n",
        "    # normalize by the number of classes\n",
        "    intra = torch.div(intra_sum, num_classes)\n",
        "\n",
        "    # compute the cumulative inter-class domain discrepancy\n",
        "    # basically we are interested in pair of classes that are different to each other\n",
        "    for c in classes:\n",
        "        for c_prime in classes:\n",
        "            if c != c_prime:\n",
        "                inter_sum = torch.add(inter_sum, domain_discrepancy(c, c_prime, phi_source, y_source, phi_target, y_target, device))\n",
        "\n",
        "    inter = torch.div(inter_sum, (num_classes*(num_classes-1)))\n",
        "\n",
        "    return torch.sub(intra, inter)\n",
        "\n",
        "\n",
        "def domain_discrepancy(c, c_prime, phi_source, y_source, phi_target, y_target, device):\n",
        "    \"\"\"\n",
        "    return the domain discrepancy between y_source and y_target according to c and c' and their activations phi\n",
        "    e1 measures the intra-class domain discrepancy in class c\n",
        "    e2 measures the intra-class domain discrepancy in class c'\n",
        "    e3 measures the inter-class domain discrepancy between c and c'\n",
        "    \"\"\"\n",
        "    intra = torch.add(\n",
        "        e(c, c, phi_source, y_source, phi_source, y_source, device),\n",
        "        e(c_prime, c_prime, phi_target, y_target, phi_target, y_target, device)\n",
        "    )\n",
        "    inter = torch.mul(-2, e(c, c_prime, phi_source, y_source, phi_target, y_target, device))\n",
        "    return torch.add(intra, inter)\n",
        "\n",
        "def e(c, c_prime, phi, y, phi_prime, y_prime, device):\n",
        "    \"\"\"\n",
        "    In order to be computationally efficient it exploits Pytorch Tensor operations,\n",
        "    instead of a straight forward implementation of the formula above with for-loops.\n",
        "\n",
        "    if c == c' measures the intra-class domain discrepancy\n",
        "    if c != c' measures the inter-class domain discrepancy\n",
        "    \"\"\"    \n",
        "    # matrix with all pairwise distances between phi and phi_prime vectors\n",
        "    kernel_covariance = rbf_kernel(phi, phi_prime)\n",
        "\n",
        "    # compute a boolean mask to filter out unwanted pairwise distances.\n",
        "    A = (y == c).unsqueeze(1).expand(-1, y_prime.size(0))\n",
        "    B = (y_prime == c_prime).unsqueeze(0)\n",
        "    mask = (A & B)\n",
        "    masked = kernel_covariance*mask\n",
        "\n",
        "    # sum the remaining similarities to then return average.\n",
        "    similarity = torch.sum(masked)\n",
        "    count = torch.count_nonzero(masked)\n",
        "\n",
        "    return torch.div(similarity, count) if count > 0 else torch.tensor(0., device=device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BqueRNKteTq"
      },
      "source": [
        "### Clustering\n",
        "In order to compute the target labels, authors proposed a clustering approach based on a Spherical K-Means algorithm.\n",
        "\n",
        "Specifically they perform what they call *Alternative optimization*. Basically they want to jointly optimize the target label hypothesis $\\hat{y}_{1:n_t}^t$ and the feature representations $\\phi_{1:L}$. At the beginning of each loop, the target labels are computed through clustering then, based on the updated target labels $\\hat{y}$, CDD is estimated and minimized to adapt the features. Model parameters $\\theta$ are updated through standard back-propagation.\n",
        "\n",
        "In order to represent a sample (on which perform clustering), they use the input activations $\\phi_1(\\cdot)$ of the first task-specific layer. In our case, each sample will be represented by the output of the global average pooling layer of a ResNet18. Then the Spherical K-Means is employed to cluster the target samples and compute the estimated labels.\n",
        "\n",
        "In order to provide a good initialization of the K-Means, for each class the target cluster center $O^{tc}$ is initialized as the source cluster center $O^{sc}$ ($O^{tc} \\gets O^{sc}$) where:\n",
        "$$\n",
        "O^{sc} = \\sum_{i=1}^{n_s} \\mathbf{1}_{y_i^s = c} \\frac{\\phi_1(x_i^s)}{||\\phi_1(x_i^s)||}, \\qquad \\mathbf{1}_{y_i^s = c} \\begin{cases}\n",
        "                                1 & \\text{ if } y_i^s= c\\\\ \n",
        "                                0 & \\text{ otherwise } \n",
        "                            \\end{cases}, \\qquad c \\in \\{1, ..., M\\}\n",
        "$$\n",
        "\n",
        "In order to measure the distance between two points in the feature space, the cosine dissimilarity is applied:\n",
        "$$\n",
        "d(a, b) = \\frac{1}{2}\\left (1 - \\frac{\\left \\langle a, b \\right \\rangle}{||a||\\,||b||}\\right )\n",
        "$$\n",
        "\n",
        "The clustering process proceeds as the classic K-Means using the just-defined distance. After clustering, each target sample is assigned to its estimated label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ljvDX9ytytM"
      },
      "outputs": [],
      "source": [
        "def compute_centroids(model, dataloader):\n",
        "    ''' \n",
        "    Forward dataloader through model, extract phi_1 (last pooling layer features) \n",
        "    and compute feature mean per class\n",
        "    return torch.Tensor with shape (num_classes, num_features)\n",
        "    '''\n",
        "    centroids = 0\n",
        "    samples_count = 0\n",
        "\n",
        "    # unsqueeze to have size (num_classes, 1) instead of simply (num_classes) in order to exploit broadcasting later\n",
        "    references = torch.tensor(range(model.num_classes), device=device).unsqueeze(1)\n",
        "    \n",
        "    labels_source = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _, (inputs, labels) in enumerate(dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels_source.extend(labels.tolist())\n",
        "            labels = labels.to(device)  # tensor of size (batch_size)\n",
        "            samples_count += labels.size(0)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            # get activations of the first task specific layer of size (batch_size, num_features)\n",
        "            features = torch.squeeze(activation[\"phi_1\"])\n",
        "\n",
        "            # resize labels tensor to (num_classes, batch_size) to use it later to generate the boolean mask\n",
        "            labels = labels.unsqueeze(0).expand(model.num_classes, -1)\n",
        "\n",
        "            # (labels == references) returns a tensor (num_classes, batch_size) thanks to broadcasting\n",
        "            # Item [c][i] in the vector is true if sample i belongs to class c\n",
        "            # By unsqueezing on last dimension mask becomes (num_classes, batch_size, 1)\n",
        "            # this is needed to compute the mask on the features exploiting again broadcasting\n",
        "            mask = (labels == references).unsqueeze(2)\n",
        "\n",
        "            # feature * mask returns a tensor (num_classes, batch_size, num_feature)\n",
        "            # where only rows on dim=1 for which the related samplelabel == class are not 0 but contain feature values\n",
        "            # by summing on dim=1 we sum feature-wise all samples belonging to a class getting a (num_classes, num_features) tensor\n",
        "            # then add the batch centroids to the centroid accumulator\n",
        "            centroids += torch.sum(features*mask, dim=1)\n",
        "    \n",
        "    # return mean centroids of the dataset\n",
        "    centroids = torch.div(centroids, samples_count)\n",
        "    return torch.nn.functional.normalize(centroids, p=2, dim=1), labels_source\n",
        "\n",
        "\n",
        "def estimate_target_labels(model, source_dl, target_dl, clustering_report=False):\n",
        "    '''\n",
        "    Returns estimated labels for target set\n",
        "    '''\n",
        "    # compute centroids, for each class, of the source domain\n",
        "    centroids, labels_source = compute_centroids(model, source_dl)\n",
        "    centroids_np = centroids.cpu().detach().numpy()\n",
        "    phi_target = []\n",
        "    labels_total = []\n",
        "\n",
        "    # get target phi_1\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, labels) in enumerate(target_dl):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            batch_phi = torch.squeeze(activation[\"phi_1\"])\n",
        "\n",
        "            phi_target.append(batch_phi)\n",
        "            labels_total.append(labels)\n",
        "\n",
        "    # create batch\n",
        "    phi_target = torch.cat(phi_target, dim=0)\n",
        "    labels_total = torch.cat(labels_total, dim=0)\n",
        "\n",
        "    phi_target = torch.squeeze(phi_target)\n",
        "    phi_target_np = phi_target.cpu().detach().numpy()\n",
        "\n",
        "    # cluster target data given centroids\n",
        "    kmeans = SphericalKMeans(n_clusters=20, init=centroids_np, n_init=1, random_state=0)\n",
        "    kmeans.fit(phi_target_np)\n",
        "    target_est_labels = kmeans.labels_\n",
        "\n",
        "    # check clustering accuracy\n",
        "    clustering_acc = accuracy_score(labels_total.to('cpu'), target_est_labels)\n",
        "    if clustering_report:\n",
        "        print(f\"Clustering accuracy: {clustering_acc}\")\n",
        "\n",
        "    return target_est_labels, phi_target, kmeans.cluster_centers_, clustering_acc, labels_source"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOlV3dnbaDqp"
      },
      "source": [
        "### Filtering\n",
        "Ambiguous data, which is far from its affiliated cluster center, is discarded. The authours deal with ambiguous data by discarding it from the computation of CDD. Data is discarded by constructing a subset of the target dataset using the following criterion:\n",
        "$$\n",
        "\\tilde{T} = \\{ (x^t, y^t) \\mid d(\\phi_1(x^t), O^{t(\\hat{y}^t)}) < D_0, x^t \\in T \\}\n",
        "$$\n",
        "where $D_0 \\in [0, 1]$ is a constant. Moreover, in order to provide more accurate estimations of the distribution statistics, they require each class to have a minimum number of samples assigned in $\\tilde{T}$. Classes which do not satisfy such condition will not be considered in current loop. At loop $T_e$ the selected subset of classes is:\n",
        "$$\n",
        "C_{T_e} = \\left \\{ c \\mid \\sum_{i}^{|\\tilde{T}|} \\mathbf{1}_{y_i^t=c} > N_0,\\, c \\in \\{ 0, ..., M-1 \\} \\right \\}\n",
        "$$\n",
        "where $N_0$ is a constant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tvawyq-zaANq"
      },
      "outputs": [],
      "source": [
        "def cosine_dissimilarity(x, x_prime):\n",
        "    # return 0.5 * (1 - cos_sim) as described in CAN paper\n",
        "    return torch.mul(0.5, torch.sub(1, F.cosine_similarity(x, x_prime, dim=0)))\n",
        "\n",
        "\n",
        "def filter_classes(phi_batch, labels, centroids, device, D_0=DEFAULT_D_0, N_0=DEFAULT_N_0):\n",
        "    \"\"\"\n",
        "    Filter the samples that does not respect the constraints.\n",
        "\n",
        "    Params:\n",
        "    -----\n",
        "    phi_batch:\n",
        "        List or tensor of activations.\n",
        "    labels:\n",
        "        List or tensor of phi_batche's labels.\n",
        "    centroids:\n",
        "        Computed target centroids\n",
        "    D_0:\n",
        "        Maximum distance from centroid\n",
        "    N_0:\n",
        "        Minimum distance for a class to be used in computing CDD.\n",
        "    \"\"\"\n",
        "\n",
        "    # keeps track of the number of occurrencies a class respects D_0 constraint\n",
        "    class_counters = [0] * 20\n",
        "\n",
        "    # init intermediate and filtered phi and labels\n",
        "    intermediate_labels = []\n",
        "    filtered_labels = []\n",
        "    intermediate_idx = []\n",
        "    filtered_idx = []\n",
        "    intermediate_phi = None\n",
        "    intermediate_phi_list = []\n",
        "    filtered_phi = None\n",
        "    filtered_phi_list = []\n",
        "\n",
        "    # apply D_0 constraint----------------------------------------\n",
        "    for i, (phi, label) in enumerate(zip(phi_batch, labels)):\n",
        "        # centroid of estimated label\n",
        "        est_centroid = centroids[label]\n",
        "        # compute distance between phi and its centroid\n",
        "        dist = cosine_dissimilarity(phi, est_centroid)\n",
        "        # check if distance respects constraint\n",
        "        if dist < D_0:\n",
        "            phi = phi.unsqueeze(0)\n",
        "            # increase the counter of the respective class\n",
        "            class_counters[label] += 1\n",
        "            intermediate_phi_list.append(phi)\n",
        "            intermediate_labels.append(label)\n",
        "            intermediate_idx.append(i)\n",
        "\n",
        "    if len(intermediate_phi_list):\n",
        "      print(\"intermediate_phi_list\")\n",
        "      intermediate_phi = torch.cat(intermediate_phi_list, dim=0)\n",
        "    \n",
        "      # apply N_0 constraint-----------------------------------------\n",
        "      for phi, label, idx in zip(intermediate_phi, intermediate_labels, intermediate_idx):\n",
        "          # check if the number of classes of label respect N_0 constraint\n",
        "          if class_counters[label] > N_0:\n",
        "              phi = phi.unsqueeze(0)\n",
        "              filtered_phi_list.append(phi)\n",
        "              filtered_labels.append(label)\n",
        "              filtered_idx.append(idx)\n",
        "\n",
        "      if len(filtered_phi_list):\n",
        "        filtered_phi = torch.cat(filtered_phi_list)\n",
        "      else:\n",
        "        filtered_phi = torch.tensor([])\n",
        "\n",
        "    # set of classes that respects constraints\n",
        "    legal_classes = set(l.item() for l in filtered_labels)\n",
        "\n",
        "    # return the filtered targets\n",
        "    return filtered_phi, torch.tensor(filtered_labels), filtered_idx, legal_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh-c9AZW9247"
      },
      "source": [
        "Class `FilteredDataset` is used to filter the dataset given a list of indexes and to match each filterd example with its estimated target label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbU2wztksHxJ"
      },
      "outputs": [],
      "source": [
        "class FilteredDataset(Dataset):\n",
        "    # mirrors torch.utils.data.Subset\n",
        "    def __init__(self, dataset, indexes, est_targets):\n",
        "        \"\"\"\n",
        "        Params:\n",
        "        -----\n",
        "        dataset:\n",
        "            Unfiltered dataset.\n",
        "        indexes:\n",
        "            List of indexes to keep track of.\n",
        "        est_targets:\n",
        "            estimated targets of filtered dataset (len(indexes) = len(est_targets)).\n",
        "        \"\"\"\n",
        "        # Compute subset of dataset\n",
        "        self.subset_ds = torch.utils.data.Subset(dataset, indexes)\n",
        "        self.labels = est_targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset_ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.subset_ds[idx][0], self.labels[idx].item()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjlz68YPaRix"
      },
      "source": [
        "### Class Aware Sampling\n",
        "A mini-batch of data is usually sampled in a class-agnostic manner. However, it will be less efficient for computing the CDD. The authors then propose a class-aware sampling to \"enable the efficient update of network with CDD\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9-2UtJ9n-VD"
      },
      "outputs": [],
      "source": [
        "class BalancedBatchSampler(BatchSampler):\n",
        "    \"\"\"\n",
        "    Sampler to get the same number of samples from different classes. \n",
        "    Adapted from https://discuss.pytorch.org/t/load-the-same-number-of-data-per-class/65198/3\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, n_classes, n_samples):\n",
        "        start = time.time()\n",
        "        loader = DataLoader(dataset)\n",
        "        self.labels_list = []\n",
        "        for _, label in loader:\n",
        "            self.labels_list.append(label)\n",
        "        del loader\n",
        "        self.labels = torch.LongTensor(self.labels_list)\n",
        "        self.labels_set = list(set(self.labels.numpy()))\n",
        "        self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n",
        "                                 for label in self.labels_set}\n",
        "        for l in self.labels_set:\n",
        "            np.random.shuffle(self.label_to_indices[l])\n",
        "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
        "        self.count = 0\n",
        "        self.n_classes = n_classes\n",
        "        self.n_samples = n_samples\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = self.n_samples * self.n_classes\n",
        "        \n",
        "\n",
        "    def __iter__(self):\n",
        "        start = time.time()\n",
        "        self.count = 0\n",
        "        while self.count + self.batch_size < len(self.dataset):\n",
        "            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
        "            indices = []\n",
        "            for class_ in classes:\n",
        "                indices.extend(self.label_to_indices[class_][\n",
        "                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
        "                                   class_] + self.n_samples])\n",
        "                self.used_label_indices_count[class_] += self.n_samples\n",
        "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
        "                    np.random.shuffle(self.label_to_indices[class_])\n",
        "                    self.used_label_indices_count[class_] = 0\n",
        "            yield indices\n",
        "            self.count += self.n_classes * self.n_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset) // self.batch_size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpx2OABEC_Nx"
      },
      "source": [
        "### StaticDataset\n",
        "A major issue we encountered, during this project, was the huge amount of time required to run CAN ($\\approx 8$ minutes per epoch). A first solution to attenuate the amount of time required was the use of masks. Masks are employed in both `e` (CDD) and `compute_centroids` (Clustering) functions. The aim is to exploit low level code, provided by pytorch, to replace for loops that otherwise would have required lots of time to run. Although they helped, time was still a major issue. After lots of tries, we found out that I/O operations (i.e. read images from the disk) were the problem. To solve this inconvenient, the idea of keeping the dataset in RAM came up to our mind. `StaticDataset`, in fact, pays a key role in reducing the time-per-epoch required by CAN. When Google Colab provides full performances the time-per-epoch is $\\approx 45$ seconds. \n",
        "\n",
        "This approach, however, bring with itself two drawbacks:\n",
        "1. Lots of RAM is used. Thus, if a cell is interrupted so the memory is not de-allocated, the runtime may require to be factory-resetted.\n",
        "2. DataLoaders with StaticDataset are created only once, but the time required is $\\approx 100$ seconds. It is not a major issue but it must be taken into consideration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tlib3pv5DY50"
      },
      "outputs": [],
      "source": [
        "class StaticDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset used to reduce the time required for training. Store the whole dataset in RAM.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset):\n",
        "        \"\"\"\n",
        "        Load the whole set (train or test) and store all of it in RAM. Used in order to avoid I/O\n",
        "        operations at each iteration.\n",
        "\n",
        "        Params:\n",
        "        -----\n",
        "        dataset: \n",
        "            train OR test set already transformed (ImageFolder + Transformations + split).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # create a DataLoader to iterate through the dataset\n",
        "        dl = DataLoader(dataset, shuffle=False, num_workers=DEFAULT_NUM_WORKERS)\n",
        "        inputs = []\n",
        "        targets = []\n",
        "        for x, y in dl:\n",
        "            inputs.append(x)\n",
        "            targets.append(y)\n",
        "        del dl\n",
        "        \n",
        "        # crate two tensors, one for inputs and one for labels\n",
        "        self.inputs = torch.cat(inputs)\n",
        "        self.targets = torch.cat(targets)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.inputs[index], self.targets[index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Skk891zul5KJ"
      },
      "source": [
        "### Training and Test step CAN\n",
        "In the training step we can first appreciate the forward of the source domain to compute the CrossEntropy Loss. Then we can see the sampling from class-aware dataloaders, through `get_samples`, in order to obtain their activations and compute the CDD loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tbxupf4wq6d"
      },
      "outputs": [],
      "source": [
        "def get_samples(dataloader, dl_iter):\n",
        "    try:\n",
        "        samples, labels = next(dl_iter)\n",
        "    except StopIteration:\n",
        "        dl_iter = iter(dataloader)\n",
        "        samples, labels = next(dl_iter)\n",
        "    return dl_iter, samples, labels\n",
        "\n",
        "\n",
        "def training_step_can(model, source_dl, source_cla_dl, target_cla_dl, optimizer, classes, device, cdd_weight=BETA):\n",
        "    cumulative_cdd_loss = 0.\n",
        "    cumulative_ce_loss = 0.\n",
        "    cumulative_loss = 0.\n",
        "    cumulative_accuracy = 0.\n",
        "\n",
        "    samples_count = 0\n",
        "    cdd_samples_count = 0\n",
        "    total_samples_count = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # init class aware iterators for CDD loss\n",
        "    if target_cla_dl is not None:\n",
        "        # source_cla_dl and target_cla_dl should have the same number of classes\n",
        "        source_cla_iter = iter(source_cla_dl)\n",
        "        target_cla_iter = iter(target_cla_dl)\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(tqdm_notebook(source_dl,  desc=\"Training step\", leave=False)):\n",
        "        # Compute crossentropy\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        ce_loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
        "        ce_loss.backward()\n",
        "\n",
        "        cdd_loss = 0\n",
        "\n",
        "        # if target_cla_dl is not None it means that there some classes survived after filtering\n",
        "        if target_cla_dl is not None:            \n",
        "            source_cla_iter, inputs_source, labels_source = get_samples(source_cla_dl, source_cla_iter)\n",
        "            target_cla_iter, inputs_target, pseudolabels_target = get_samples(target_cla_dl, target_cla_iter)\n",
        "\n",
        "            # move to device\n",
        "            inputs_source, labels_source = inputs_source.to(device), labels_source.to(device)\n",
        "            inputs_target, pseudolabels_target = inputs_target.to(device), pseudolabels_target.to(device)\n",
        "\n",
        "            outputs_source = model(inputs_source)\n",
        "            # list of activations\n",
        "            #   - global average pool\n",
        "            #   - output layer\n",
        "            source_phis = [activation['phi_1'].squeeze(), outputs_source]\n",
        "\n",
        "            outputs_target = model(inputs_target)\n",
        "            target_phis = [activation['phi_1'].squeeze(), outputs_target]\n",
        "\n",
        "            cdd_loss = CDD_loss(source_phis, target_phis, labels_source, pseudolabels_target, classes, cdd_weight) # already weighted by cdd_weight\n",
        "            \n",
        "            # backward pass for CDD Loss\n",
        "            cdd_loss.backward()\n",
        "            cdd_samples_count += inputs_source.shape[0]\n",
        "\n",
        "        samples_count += inputs.shape[0]\n",
        "        total_samples_count += samples_count + cdd_samples_count\n",
        "\n",
        "        loss = ce_loss + cdd_loss # already weighted\n",
        "\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # reset the optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        cumulative_ce_loss += ce_loss\n",
        "        cumulative_cdd_loss += cdd_loss\n",
        "        cumulative_loss += loss\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        cumulative_accuracy += predicted.eq(labels).sum().item()\n",
        "\n",
        "    ce = cumulative_ce_loss / samples_count\n",
        "    cdd = cumulative_cdd_loss / cdd_samples_count if cdd_samples_count > 0 else 0\n",
        "    loss = cumulative_loss / total_samples_count\n",
        "    acc = cumulative_accuracy / samples_count\n",
        "\n",
        "    metrics = {\n",
        "        \"train/train_ce\": ce,\n",
        "        \"train/train_cdd\": cdd,\n",
        "        \"train/train_loss\": loss,\n",
        "        \"train/train_acc\": acc\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def test_step_can(model, data_loader, cost_function, device, epoch=0):\n",
        "    n_samples = 0\n",
        "    cumulative_loss = 0.\n",
        "    cumulative_accuracy = 0.\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(tqdm_notebook(data_loader, desc=\"Test Step\", leave=False)):\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = cost_function(outputs, targets)\n",
        "\n",
        "            # cumulative loss\n",
        "            n_samples += inputs.shape[0]  # add batch size\n",
        "            cumulative_loss += loss.item()\n",
        "            max_prob, predicted = outputs.max(dim=1)  # return predicted labels\n",
        "\n",
        "            # cumulative accuracy\n",
        "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "            # log to wandb\n",
        "            metrics = {\"test/test_loss\": cumulative_loss/n_samples,\n",
        "                       \"test/test_acc\": cumulative_accuracy/n_samples}\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDxV8JHcE0Go"
      },
      "source": [
        "### Training Loop CAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP-96qga4Wtp"
      },
      "outputs": [],
      "source": [
        "def create_class_aware_dataloader(dataset, classes):\n",
        "    \"\"\"\n",
        "    Return a dataloader, with batch_sampler=BalancedBatchSampler, on the :dataset: with the selected :classes:.\n",
        "    \"\"\"\n",
        "    # compute number of samples per class\n",
        "    n_labels = torch.zeros(len(classes))\n",
        "\n",
        "    # mapping from class indexes to actual indexes of the list\n",
        "    # e.g. classes = {3, 5, 6}\n",
        "    # map_ = {\n",
        "    #   '3': 0,\n",
        "    #   '5': 1,\n",
        "    #   '6': 2\n",
        "    #}\n",
        "    map_ = {cls: idx for idx, cls in enumerate(classes)}\n",
        "\n",
        "    # count the number of occurrences for each class in the filtered dataset\n",
        "    for _, target in dataset:\n",
        "        if torch.is_tensor(target):\n",
        "            target = target.item()\n",
        "        n_labels[map_[target]] += 1\n",
        "\n",
        "    # get class with minimum number of samples\n",
        "    min_samples = torch.min(n_labels).item()\n",
        "\n",
        "    # update samples per class\n",
        "    # In absence of guidance, we selected 3 samples_per_class as upper bound.\n",
        "    # This is motivated by the fact that 3 * 20 (max number of classes) = 60\n",
        "    # which is a reasonable dimension. Moreover, it does not overload too much\n",
        "    # the computation.\n",
        "    samples_per_class = min(3, min_samples)\n",
        "    sampler = BalancedBatchSampler(dataset, len(classes), samples_per_class)\n",
        "    return DataLoader(dataset, batch_sampler=sampler, num_workers=DEFAULT_NUM_WORKERS, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-94ymBBaK6FX"
      },
      "outputs": [],
      "source": [
        "def save_weights(epoch, model, optimizer, loss, path, scheduler=None):\n",
        "    save_dict = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler': scheduler.state_dict() if scheduler is not None else None\n",
        "    }\n",
        "    torch.save(save_dict, path)\n",
        "\n",
        "\n",
        "def load_weights(model, optimizer, weights_path, device, scheduler=None):\n",
        "    checkpoint = torch.load(weights_path, map_location=device)\n",
        "    epoch = checkpoint['epoch']\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if scheduler != None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "\n",
        "    return epoch, model, optimizer, scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4GNF06Ehd2y"
      },
      "source": [
        "Here comes the training loop. In the first two lines we can see the network initialization and the addition of a hook. The latter is used to grab the activations of intermediate layers in the network.\n",
        "\n",
        "Proceeding we can appreciate the retrieval of a scheduler in case `sgd` is used as optimizer. This feature has been implemented for testing purposes (since Adam is way faster in converging and helped us to reduce time while developing). \n",
        "\n",
        "`StaticDatasets` are then initialized. You may have noticed that only 3 out of 4 datasets are taken into consideration. This is due to the fact that the source test set is not used during training.\n",
        "\n",
        "`DataLoaders` used for the computation of CDD aren't shuffled as it is not required. Moreover, not shuffling the dataset makes the code much simpler.\n",
        "\n",
        "Then, we can see CAN algorithm:\n",
        "1. Estimation of target labels;\n",
        "2. Class filtering;\n",
        "3. Computation of the source dataset subset based on the filtered classes;\n",
        "4. Computation of the `FilteredDataset` on the target dataset in order to filter it and applying it the estimated labels;\n",
        "5. Creation of the Class-Aware DataLoaders;\n",
        "6. Training step\n",
        "7. Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_Y15S90ywnx"
      },
      "outputs": [],
      "source": [
        "def training_loop_can(source_tr_ds, target_ts_ds, target_tr_ds, device, wandb_run, weights=None, save=False):\n",
        "    # instatiate model\n",
        "    model = ResNet18(NUM_CLASSES).to(device)\n",
        "    model.backbone.avgpool.register_forward_hook(get_activation('phi_1'))  # add hook to extract activations\n",
        "    \n",
        "    epochs = wandb.config['epochs']\n",
        "\n",
        "    optimizer = get_optimizer(model, lr=wandb.config[\"lr\"], optim=wandb.config[\"optimizer\"])\n",
        "    scheduler = None\n",
        "    if (wandb.config[\"optimizer\"] == \"sgd\"):\n",
        "        scheduler = get_lr_scheduler(optimizer, epochs)\n",
        "\n",
        "    experiment = wandb_run.name\n",
        "\n",
        "    # used to resume epoch after load weights\n",
        "    last_epoch = 0\n",
        "    if weights != None:\n",
        "        last_epoch, model, optimizer, scheduler = load_weights(model, optimizer, weights, device, scheduler)\n",
        "        print(f\"Weights restored from epoch {last_epoch}\")\n",
        "    \n",
        "    print(experiment)\n",
        "\n",
        "    # static datasets\n",
        "    source_tr_static = StaticDataset(source_tr_ds)\n",
        "    target_ts_static = StaticDataset(target_ts_ds)\n",
        "    target_tr_static = StaticDataset(target_tr_ds)\n",
        "    \n",
        "    # for ce\n",
        "    source_tr_dl = DataLoader(source_tr_static, BATCH_SIZE, shuffle=True, num_workers=DEFAULT_NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "    # for cdd \n",
        "    target_ts_dl = DataLoader(target_ts_static, BATCH_SIZE, shuffle=False, num_workers=DEFAULT_NUM_WORKERS, pin_memory=True)\n",
        "    tr_dl_source_unshuff = DataLoader(source_tr_static, BATCH_SIZE, shuffle=False, num_workers=DEFAULT_NUM_WORKERS, pin_memory=True)\n",
        "    tr_dl_target_unshuff = DataLoader(target_tr_static, BATCH_SIZE, shuffle=False, num_workers=DEFAULT_NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "    print(\"Start Training\")\n",
        "    for e in tqdm_notebook(range(last_epoch, epochs), desc=\"Training loop\"):\n",
        "        # Clustering\n",
        "        target_est_labels, phi_target, centroids, clustering_acc, source_labels = estimate_target_labels(model, tr_dl_source_unshuff, tr_dl_target_unshuff, clustering_report=True)\n",
        "\n",
        "        # Filtering classes\n",
        "        _, target_pseudo_labels, target_idxs, classes = filter_classes(phi_target, target_est_labels, torch.tensor(centroids).to(device), device, D_0=wandb.config[\"D_0\"], N_0=wandb.config[\"N_0\"])\n",
        "\n",
        "        # iterates through source labels, keeps only the indexes wich label is in classes set\n",
        "        source_idxs = [i for i, y in enumerate(source_labels) if y in classes]\n",
        "\n",
        "        # Define Filtered Datasets\n",
        "        source_filt_ds = Subset(source_tr_static, source_idxs)\n",
        "        print(f\"Source filtered dataset lenght: {len(source_filt_ds)}\")\n",
        "\n",
        "        target_filt_ds = FilteredDataset(target_tr_static, target_idxs, target_pseudo_labels)\n",
        "        print(f\"Target filtered dataset lenght: {len(target_filt_ds)}\")\n",
        "\n",
        "        if len(classes) < 2:\n",
        "            print(\"No classes after filtering\")\n",
        "            source_cla_dl = None\n",
        "            target_cla_dl = None\n",
        "        else:\n",
        "            # Define C-A Dataloader\n",
        "            source_cla_dl = create_class_aware_dataloader(source_filt_ds, classes)\n",
        "            target_cla_dl = create_class_aware_dataloader(target_filt_ds, classes)\n",
        "\n",
        "        train_metrics = training_step_can(model, source_tr_dl, source_cla_dl, target_cla_dl, optimizer, classes, device)\n",
        "\n",
        "        test_metrics = test_step_can(model, target_ts_dl, torch.nn.CrossEntropyLoss(), device)\n",
        "        \n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Log to WandB\n",
        "        metrics = {**train_metrics,\n",
        "                   'train/filtered_classes': len(classes),\n",
        "                   'train/clustering_acc': clustering_acc,\n",
        "                   **test_metrics}\n",
        "        wandb.log(metrics)\n",
        "\n",
        "        train_loss = train_metrics['train/train_loss']\n",
        "        train_ce = train_metrics['train/train_ce']\n",
        "        train_cdd = train_metrics['train/train_cdd']\n",
        "        train_acc = train_metrics['train/train_acc']\n",
        "\n",
        "        test_loss = test_metrics['test/test_loss']\n",
        "        test_acc = test_metrics['test/test_acc']\n",
        "\n",
        "        new_best = False\n",
        "        if e == 0 or best_acc < test_acc:\n",
        "            best_acc = test_acc\n",
        "            best_loss = test_loss\n",
        "            best_model = copy.deepcopy(model)\n",
        "            new_best = True\n",
        "\n",
        "            # Save new best weights\n",
        "            if save:\n",
        "                save_weights(e, model, optimizer, test_loss, f'/content/drive/MyDrive/weights/ResNet18CAN_{experiment}.pth', scheduler)\n",
        "                artifact = wandb.Artifact(f'ResNet18CAN_{experiment}', type='model', metadata={**wandb_run.config, **metrics})\n",
        "                artifact.add_file(f'/content/drive/MyDrive/weights/ResNet18CAN_{experiment}.pth')\n",
        "                wandb_run.log_artifact(artifact)\n",
        "\n",
        "        print('\\n Epoch: {:d}'.format(e + 1))\n",
        "        print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_acc))\n",
        "        print('\\t Training CE {:.5f}, Training CDD {:.5f}'.format(train_ce, train_cdd))\n",
        "        print('\\t Training filtered classes {}'.format(len(classes)))\n",
        "        print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_acc))\n",
        "        if new_best:\n",
        "            print(\"\\t New best weights saved\")\n",
        "        print('-----------------------------------------------------')\n",
        "    \n",
        "    visualize(best_model, target_ts_dl, wandb_run)\n",
        "    wandb.summary[\"test_best_loss\"] = best_loss\n",
        "    wandb.summary[\"test_best_accuracy\"] = best_acc\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "    del source_tr_static\n",
        "    del target_ts_static\n",
        "    del target_tr_static\n",
        "\n",
        "    print('\\t BEST Test loss {:.5f}, Test accuracy {:.2f}'.format(best_loss, best_acc))\n",
        "\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gUW8prA8pYw"
      },
      "source": [
        "### Experiments\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3YS6fZValgR"
      },
      "source": [
        "Train model on Products, test on Real World"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0p22yCwkpXEU"
      },
      "outputs": [],
      "source": [
        "wandb_run = wandb.init(\n",
        "    project=\"DL2022_229356_229298\",\n",
        "    entity=\"229356_229298\",\n",
        "    name=\"P_to_R_can\",\n",
        "    config={\n",
        "        \"model\": \"ResNet18CAN\",\n",
        "        \"trained-on\": \"Source + Target unsupervised\",\n",
        "        \"epochs\": 60,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"optimizer\": \"sgd\",\n",
        "        \"lr\": 1e-2,\n",
        "        \"D_0\": 0.075,\n",
        "        \"N_0\": DEFAULT_N_0\n",
        "    }\n",
        ")\n",
        "\n",
        "best_model = training_loop_can(train_products, test_real, train_real, device, wandb_run, save=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMVuH3jNas4r"
      },
      "source": [
        "**Best test accuracy $P \\rightarrow R:$ 0.79**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_luSoO1oRimV"
      },
      "source": [
        "class | precision  |  recall | f1-score  | support\n",
        "------|------------|---------|-----------|---------\n",
        "backpack       |0.53      |0.94      |0.68        |17\n",
        "bookcase       |0.78      |0.70      |0.74        |20\n",
        "car jack       |0.64      |0.93      |0.76        |15\n",
        "comb       |0.77      |0.77      |0.77        |22\n",
        "crown       |0.76      |0.90      |0.83        |21\n",
        "file cabinet       |0.59      |0.73      |0.65        |22\n",
        "flat iron       |0.82      |0.88      |0.85        |16\n",
        "game controller       |0.94      |0.71      |0.81        |21\n",
        "glasses       |1.00      |0.68      |0.81        |19\n",
        "helicopter       |1.00      |0.74      |0.85        |19\n",
        "ice skates       |0.69      |0.86      |0.77        |21\n",
        "letter tray       |0.73      |0.70      |0.72        |27\n",
        "monitor       |0.79      |0.75      |0.77        |20\n",
        "mug       |1.00      |0.92      |0.96        |24\n",
        "network switch       |0.91      |0.59      |0.71        |17\n",
        "over-ear headphones       |0.94      |1.00      |0.97        |17\n",
        "pen       |1.00      |0.76      |0.87        |17\n",
        "purse       |0.67      |0.61      |0.64        |23\n",
        "stand mixer       |0.91      |0.95      |0.93        |21\n",
        "stroller       |0.89      |0.76      |0.82        |21\n",
        "||||\n",
        "accuracy      |           |          |0.79       |400\n",
        "macro avg     |  0.82     | 0.79      |0.79       |400\n",
        "weighted avg  |     0.82  |    0.79      |0.79       |400"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7BV-i00c_L6"
      },
      "outputs": [],
      "source": [
        "wandb_run = wandb.init(\n",
        "    project=\"DL2022_229356_229298\",\n",
        "    entity=\"229356_229298\",\n",
        "    name=\"R_to_P_can\",\n",
        "    config={\n",
        "        \"model\": \"ResNet18CAN\",\n",
        "        \"trained-on\": \"Source + Target unsupervised\",\n",
        "        \"epochs\": 60,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"optimizer\": \"sgd\",\n",
        "        \"lr\": 1e-2,\n",
        "        \"D_0\": 0.075,\n",
        "        \"N_0\": DEFAULT_N_0\n",
        "    }\n",
        ")\n",
        "\n",
        "best_model = training_loop_can(train_real, test_products, train_products, device, wandb_run, save=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fAdEYZNa6IG"
      },
      "source": [
        "**Best test accuracy $R \\rightarrow P:$ 0.94**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uv8Gcih6DqC"
      },
      "source": [
        "|      class         |precision  |  recall  |f1-score  | support |\n",
        "|---|---|---|---|---|\n",
        "|           backpack |      0.97 |     0.97 |     0.97 |       29|\n",
        "|           bookcase |      0.83 |     0.90 |     0.86 |       21|\n",
        "|           car jack |      0.94 |     0.94 |     0.94 |       17|\n",
        "|               comb |      0.95 |     0.95 |     0.95 |       19|\n",
        "|              crown |      1.00 |     1.00 |     1.00 |       20|\n",
        "|       file cabinet |      0.88 |     0.78 |     0.82 |       18|\n",
        "|          flat iron |      0.94 |     0.94 |     0.94 |       16|\n",
        "|    game controller |      0.95 |     0.88 |     0.91 |       24|\n",
        "|            glasses |      0.95 |     1.00 |     0.97 |       19|\n",
        "|         helicopter |      0.94 |     1.00 |     0.97 |       17|\n",
        "|         ice skates |      1.00 |     0.95 |     0.97 |       19|\n",
        "|        letter tray |      0.83 |     0.94 |     0.88 |       16|\n",
        "|            monitor |      0.95 |     0.90 |     0.92 |       20|\n",
        "|                mug |      0.94 |     1.00 |     0.97 |       17|\n",
        "|     network switch |      0.92 |     0.96 |     0.94 |       24|\n",
        "|over-ear headphones |      0.83 |     1.00 |     0.91 |       15|\n",
        "|                pen |      0.92 |     0.83 |     0.87 |       29|\n",
        "|              purse |      0.95 |     0.86 |     0.90 |       21|\n",
        "|        stand mixer |      1.00 |     1.00 |     1.00 |       19|\n",
        "|           stroller |      1.00 |     1.00 |     1.00 |       20|\n",
        "| | | | |\n",
        "|           accuracy |           |          |     0.94 |      400|\n",
        "|          macro avg |      0.93 |     0.94 |     0.94 |      400|\n",
        "|       weighted avg |      0.94 |     0.94 |     0.93 |      400|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYDz06oZ5wKA"
      },
      "source": [
        "### Observations\n",
        "We can observe that the effect of unsupervised domain adaptation through Contrastive Adaptation Network are noticeable. Overall accuracy increases in both directions. \n",
        "This is clearly due to the effect of the CDD Loss which allows to perform better domain alignment comparing to the baseline approach. We may notice also that the CDD behaves like a sort of regularizer, reducing overfitting. As we can see from the plots below, even when training accuracy is 1, loss minimization (Crossentropy + $\\beta$ CDD) continues as well as the increase in test accuracy. As the authors suggest, \"maximizing the inter-class domain discrepancy may alleviate the possibility of the model overfitting to the source data and benefits the adaptation\". Therefore we expect that with longer training the fine tuning of the network could make us reach slightly higher values in terms of accuracy. \n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1FaOkrprdEX_Y4sfSiqcL3TPFLuzBuxBF\" width=\"500px\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1qU2XQ7WQxnxAHBlHXST9s9y4Hu81neXi\" width=\"500px\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1lQGxxxNjfg0ZppBD94W1hYfgCiMYgdPW\" width=\"500px\">\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1w4joWo8oZx-3WssXIt7ZygKs7jMydtwX\" width=\"500px\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1v89OtDB2aYqvhf_s41E34XOn4TqZfc6l\" width=\"500px\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1OAOSlx4PJvZLD1G4hzj0UQZHCVK9Rzjl\" width=\"500px\">\n",
        "\n",
        "Moreover we can observe that the clustering accuracy, which is the accuracy on the predicted target pseudo-labels, sligthly increases throughout the training thanks to the CDD. Feature representations of samples are more compact among the same class due to the intra class domain discrepancy minimization and further away among different classes thanks to the inter class domain discrepancy maximization.\n",
        "\n",
        "In the case of Products to Real Word direction, the number of classes remaining after the filtering step behaves as expected. At the beginning of the training the number of classes remaining is low and increases as the CDD loss is minimized. In order to obtain this behaviour we needed to slightly tweak $D_0$ hyper-parameter to $0.075$ instead of $0.05$ used by the authors. Default $D_0$ value did not allow enough samples to be kept, obtaining as a consequence zero classes after filtering, making impossible the computation of CDD loss. $N_0$ was left to $3$. \n",
        "\n",
        "In the opposite direction of the UDA task this behaviour is not observed since the domains are already well aligned thanks to the pretraining of the backbone on ILSVRC dataset. Therefore, since the beginning of the training, all 20 classes are used for the computation of the CDD.\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1_JTeQxh0UTrFwCV8TC2FtuCpxy10fN6e\" width=\"500px\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1mBdRO-dlAQm6F7tZnnZePywsjqrTq8aQ\" width=\"500px\">\n",
        "\n",
        "In terms of the **gain** obtained by applying CAN, we got the following results:\n",
        "\n",
        "| Method | Acc P -> R (%) | Acc R -> P (%) |\n",
        "|---|---|---|\n",
        "| *Upper bound* |  *92.0*  |  *95.0*  |\n",
        "| Baseline      |   74.5   |   92.5   |\n",
        "| CAN           |   79.0   |   93.5   |\n",
        "| **Gain CAN**  | **4.5** | **1.0** |\n",
        "\n",
        "\n",
        "Those results were expected and are coherent with those presentend in Adaptiope article modulo the fact that for this project we are dealing with 20 classes instead of 123. Moreover, we used a ResNet18 while, in the literature, CAN is implemented through ResNet50 or ResNet101.\n",
        "We acknoledge that and, based on the upper bounds we computed, there is room for improvements for both directions of the UDA task, expecially for Products to Real World direction. As we can see the confusion matrix and the t-SNE 2D visualization are still inaccurate and noisy. However, compared to the baseline, CAN demonstrates higher intra-class compactness and much larger inter-class margin.\n",
        "\n",
        "<div><b>Products to Real World</b> confusion matrix and t-SNE scatter plot: <br>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1CZwSTv2EWenahP6O73o3X37HX4IZi3jw\" height=\"400px\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1TejKrBi7LLcawX5UZQ8fAoTru7F1m0X1\" height=\"300px\">\n",
        "</div>\n",
        "\n",
        "\n",
        "Furthermore we note that this vanilla implementation of CAN is very slow in converging due to Stochastic Gradient Descent optimization algorithm. Therefore, many epochs are needed to get good results. Clearly training with plain SGD plus a learning rate scheduler is not the best choiche. An adaptive optimization algorithm like Adam would help CAN converging faster to a better local optimum. \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YFojsqhVAKV"
      },
      "source": [
        "## Improving CAN\n",
        "The vanilla implementation of CAN it is very slow in converging. Moreover, it uses 512 features to compute clustering (using ResNet18) and K-Means is subject to the \"curse of dimensionality\". To be more precise, distances in general are subject to this issue.\n",
        "\n",
        "In the following sections we will propose our improvements in order to make CAN converging faster and slightly more accurate.\n",
        "\n",
        "> Note: Since this section is very similar to the CAN one (referring to code), in order to make the notebook more compact, much of CAN code is used also here. Thus, make sure to run each part of CAN (except for experiments) in order to have everything needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph23pwKWMRei"
      },
      "source": [
        "### Better Clustering\n",
        "As just mentioned, K-Means clustering is subject to the \"curse of dimensionality\". Generally speaking when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. Therefore, distance based algorithm like K-Means tend to perform poorly.\n",
        "\n",
        "In order to attenuate the negative effects of the large amount of features, we decided to adopt a Kernel PCA to reduce substantially the number of features. Basically we compute the Kernel PCA to source and target domain together, and we use the obtained lower dimensional features to compute the Spherical K-Means. Moreover, we adopt it also to compute the CDD loss, as cosine distance is also affected by the \"curse of dimensionality\". Additionally, by applying feature reduction, we try to compact the feature representation in order to reduce noise in the data.\n",
        "\n",
        "The choice of using [Kernel PCA](https://www.face-rec.org/algorithms/Kernel/kernelPCA_scholkopf.pdf) feature reduction algorithm, instead of linear PCA, is motivated by the fact that feature reduction through non-linear transformations is better suited for dealing with non linearly separable data, like the features obtained by the CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_igPuSdMa7G"
      },
      "outputs": [],
      "source": [
        "def compute_centroids_improvements(model, dataloader):\n",
        "    ''' \n",
        "    Forward dataloader through model, extract phi_1 (last pooling layer features) \n",
        "    and compute feature mean per class\n",
        "    return torch.Tensor with shape (num_classes, num_features)\n",
        "    '''\n",
        "    centroids = 0\n",
        "    samples_count = 0\n",
        "\n",
        "    # unsqueeze to have size (num_classes, 1) instead of simply (num_classes) in order to exploit broadcasting later\n",
        "    references = torch.tensor(range(model.num_classes), device=device).unsqueeze(1)\n",
        "    \n",
        "    labels_source = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _, (inputs, labels) in enumerate(dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels_source.extend(labels.tolist())\n",
        "            labels = labels.to(device)  # tensor of size (batch_size)\n",
        "            samples_count += labels.size(0)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            # get activations of the first task specific layer of size (batch_size, num_features)\n",
        "            features = torch.squeeze(activation[\"phi_1\"])\n",
        "\n",
        "            # resize labels tensor to (num_classes, batch_size) to use it later to generate the boolean mask\n",
        "            labels = labels.unsqueeze(0).expand(model.num_classes, -1)\n",
        "\n",
        "            # (labels == references) returns a tensor (num_classes, batch_size) thanks to broadcasting\n",
        "            # Item [c][i] in the vector is true if sample i belongs to class c\n",
        "            # By unsqueezing on last dimension mask becomes (num_classes, batch_size, 1)\n",
        "            # this is needed to compute the mask on the features exploiting again broadcasting\n",
        "            mask = (labels == references).unsqueeze(2)\n",
        "\n",
        "            # feature * mask returns a tensor (num_classes, batch_size, num_feature)\n",
        "            # where only rows on dim=1 for which the related samplelabel == class are not 0 but contain feature values\n",
        "            # by summing on dim=1 we sum feature-wise all samples belonging to a class getting a (num_classes, num_features) tensor\n",
        "            # then add the batch centroids to the centroid accumulator\n",
        "            centroids += torch.sum(features*mask, dim=1)\n",
        "    \n",
        "    # return mean centroids of the dataset\n",
        "    centroids = torch.div(centroids, samples_count)\n",
        "    return torch.nn.functional.normalize(centroids, p=2, dim=1), labels_source, features\n",
        "\n",
        "def estimate_target_labels_improvements(model, source_dl, target_dl, clustering_report=False):\n",
        "    '''\n",
        "    Returns estimated labels for target set in order\n",
        "    '''\n",
        "    # compute centroids, for each class, of the source domain\n",
        "    centroids, labels_source, features = compute_centroids_improvements(model, source_dl)\n",
        "    centroids_np = centroids.cpu().detach().numpy()\n",
        "\n",
        "    phi_target = []\n",
        "    labels_total = []\n",
        "\n",
        "    # get target phi_1\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, labels) in enumerate(target_dl):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            batch_phi = torch.squeeze(activation[\"phi_1\"])\n",
        "\n",
        "            phi_target.append(batch_phi)\n",
        "            labels_total.append(labels)\n",
        "\n",
        "    phi_target = torch.cat(phi_target, dim=0)\n",
        "    labels_total = torch.cat(labels_total, dim=0)\n",
        "\n",
        "    phi_target = torch.squeeze(phi_target)\n",
        "    phi_target_np = phi_target.cpu().detach().numpy()\n",
        "\n",
        "    features_np = features.cpu().detach().numpy()\n",
        "\n",
        "    # dim reduction\n",
        "    kpca = KernelPCA(80, kernel='rbf')\n",
        "    kpca.fit(np.concatenate((phi_target_np, features_np)))\n",
        "    phi_target_np = kpca.transform(phi_target_np)\n",
        "    centroids_np = kpca.transform(centroids_np)\n",
        "\n",
        "    phi_target = torch.tensor(phi_target_np, device=device)\n",
        "\n",
        "    # cluster target data given centroids\n",
        "    kmeans = SphericalKMeans(n_clusters=20, init=centroids_np, n_init=1, random_state=0)\n",
        "    kmeans.fit(phi_target_np)\n",
        "    target_est_labels = kmeans.labels_\n",
        "\n",
        "    # check clustering accuracy\n",
        "    clustering_acc = accuracy_score(labels_total.to('cpu'), target_est_labels)\n",
        "    if clustering_report:\n",
        "        print(f\"Clustering accuracy: {clustering_acc}\")\n",
        "\n",
        "    return target_est_labels, phi_target, kmeans.cluster_centers_, clustering_acc, labels_source, kpca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCWjyEtADwQU"
      },
      "source": [
        "### Training Step Improvements\n",
        "This code is the same as above but, here we can see the application on the Kernel PCA in the central part of the for loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNLY3X13D14H"
      },
      "outputs": [],
      "source": [
        "def apply_kpca(batch_phi, kpca):\n",
        "    batch_phi = batch_phi.squeeze().cpu().numpy()\n",
        "    batch_phi = kpca.transform(batch_phi)\n",
        "    return torch.tensor(batch_phi).to(device)\n",
        "\n",
        "def training_step_improvements(model, source_dl, source_cla_dl, target_cla_dl, optimizer, classes, kpca, device, cdd_weight=BETA):\n",
        "    cumulative_cdd_loss = 0.\n",
        "    cumulative_ce_loss = 0.\n",
        "    cumulative_loss = 0.\n",
        "    cumulative_accuracy = 0.\n",
        "\n",
        "    samples_count = 0\n",
        "    cdd_samples_count = 0\n",
        "    total_samples_count = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # init class aware iterators for CDD loss\n",
        "    if target_cla_dl is not None:\n",
        "        # source_cla_dl and target_cla_dl should have the same number of classes\n",
        "        source_cla_iter = iter(source_cla_dl)\n",
        "        target_cla_iter = iter(target_cla_dl)\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(tqdm_notebook(source_dl,  desc=\"Training step\", leave=False)):\n",
        "        # Compute crossentropy\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        ce_loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
        "        ce_loss.backward()\n",
        "\n",
        "        cdd_loss = 0\n",
        "\n",
        "        # if target_cla_dl is not None it means that there some classes remain after filtering\n",
        "        if target_cla_dl is not None:            \n",
        "            source_cla_iter, inputs_source, labels_source = get_samples(source_cla_dl, source_cla_iter)\n",
        "            target_cla_iter, inputs_target, pseudolabels_target = get_samples(target_cla_dl, target_cla_iter)\n",
        "\n",
        "            # move to device\n",
        "            inputs_source, labels_source = inputs_source.to(device), labels_source.to(device)\n",
        "            inputs_target, pseudolabels_target = inputs_target.to(device), pseudolabels_target.to(device)\n",
        "\n",
        "            outputs_source = model(inputs_source)\n",
        "            source_phis = apply_kpca(activation['phi_1'], kpca)\n",
        "            \n",
        "            outputs_target = model(inputs_target)\n",
        "            target_phis = apply_kpca(activation['phi_1'], kpca)\n",
        "\n",
        "            source_phis = [source_phis, outputs_source]\n",
        "            target_phis = [target_phis, outputs_target]\n",
        "\n",
        "            cdd_loss = CDD_loss(source_phis, target_phis, labels_source, pseudolabels_target, classes, cdd_weight) # already weighted by cdd_weight\n",
        "            \n",
        "            # backward pass for CDD Loss\n",
        "            cdd_loss.backward()\n",
        "            cdd_samples_count += inputs_source.shape[0]\n",
        "\n",
        "        samples_count += inputs.shape[0]\n",
        "        total_samples_count += samples_count + cdd_samples_count\n",
        "\n",
        "        loss = ce_loss + cdd_loss # already weighted\n",
        "\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # reset the optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        cumulative_ce_loss += ce_loss\n",
        "        cumulative_cdd_loss += cdd_loss\n",
        "        cumulative_loss += loss\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        cumulative_accuracy += predicted.eq(labels).sum().item()\n",
        "\n",
        "    ce = cumulative_ce_loss / samples_count\n",
        "    cdd = cumulative_cdd_loss / cdd_samples_count if cdd_samples_count > 0 else 0\n",
        "    loss = cumulative_loss / total_samples_count\n",
        "    acc = cumulative_accuracy / samples_count\n",
        "\n",
        "    metrics = {\n",
        "        \"train/train_ce\": ce,\n",
        "        \"train/train_cdd\": cdd,\n",
        "        \"train/train_loss\": loss,\n",
        "        \"train/train_acc\": acc\n",
        "    }\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_FKFVCQ270j"
      },
      "source": [
        "### Training Loop with Improvements\n",
        "Also here the code is basically the same as Vanilla CAN but `kpca` is returned from `estimate_target_labels` and passed to `training_step_improvements`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHFc_BPD2MmY"
      },
      "outputs": [],
      "source": [
        "def training_loop_improvements(source_tr_ds, target_ts_ds, target_tr_ds, device, wandb_run, weights=None, save=False):\n",
        "    # instatiate model\n",
        "    model = ResNet18(NUM_CLASSES).to(device)\n",
        "    model.backbone.avgpool.register_forward_hook(get_activation('phi_1'))  # add hook to extract activations\n",
        "    \n",
        "    epochs = wandb.config['epochs']\n",
        "\n",
        "    optimizer = get_optimizer(model, lr=wandb.config[\"lr\"], optim=wandb.config[\"optimizer\"])\n",
        "    scheduler = None\n",
        "    if (wandb.config[\"optimizer\"] == \"sgd\"):\n",
        "        scheduler = get_lr_scheduler(optimizer, epochs)\n",
        "\n",
        "    experiment = wandb_run.name\n",
        "\n",
        "    # used to resume epoch after load weights\n",
        "    last_epoch = 0\n",
        "    if weights != None:\n",
        "        last_epoch, model, optimizer, scheduler = load_weights(model, optimizer, weights, device, scheduler)\n",
        "        print(f\"Weights restored from epoch {last_epoch}\")\n",
        "    \n",
        "    print(experiment)\n",
        "\n",
        "    # static datasets\n",
        "    source_tr_static = StaticDataset(source_tr_ds)\n",
        "    target_ts_static = StaticDataset(target_ts_ds)\n",
        "    target_tr_static = StaticDataset(target_tr_ds)\n",
        "    \n",
        "    # for ce\n",
        "    source_tr_dl = DataLoader(source_tr_static, BATCH_SIZE, shuffle=True, num_workers=DEFAULT_NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "    # for cdd \n",
        "    target_ts_dl = DataLoader(target_ts_static, BATCH_SIZE, shuffle=False, num_workers=DEFAULT_NUM_WORKERS, pin_memory=True)\n",
        "    tr_dl_source_unshuff = DataLoader(source_tr_static, BATCH_SIZE, shuffle=False, num_workers=DEFAULT_NUM_WORKERS, pin_memory=True)\n",
        "    tr_dl_target_unshuff = DataLoader(target_tr_static, BATCH_SIZE, shuffle=False, num_workers=DEFAULT_NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "    print(\"Start Training\")\n",
        "    for e in tqdm_notebook(range(last_epoch, epochs), desc=\"Training loop\"):\n",
        "        # Clustering\n",
        "        target_est_labels, phi_target, centroids, clustering_acc, source_labels, kpca = estimate_target_labels_improvements(model, tr_dl_source_unshuff, tr_dl_target_unshuff, clustering_report=True)\n",
        "\n",
        "        # Filtering classes\n",
        "        _, target_pseudo_labels, target_idxs, classes = filter_classes(phi_target, target_est_labels, torch.tensor(centroids).to(device), device, D_0=wandb.config[\"D_0\"], N_0=wandb.config[\"N_0\"])\n",
        "\n",
        "        # iterates through source labels, keeps only the indexes wich label is in classes set\n",
        "        source_idxs = [i for i, y in enumerate(source_labels) if y in classes]\n",
        "\n",
        "        # Define Filtered Datasets\n",
        "        source_filt_ds = Subset(source_tr_static, source_idxs)\n",
        "        print(f\"Source filtered dataset lenght: {len(source_filt_ds)}\")\n",
        "\n",
        "        target_filt_ds = FilteredDataset(target_tr_static, target_idxs, target_pseudo_labels)\n",
        "        print(f\"Target filtered dataset lenght: {len(target_filt_ds)}\")\n",
        "\n",
        "        if len(classes) < 2:\n",
        "            print(\"No classes after filtering\")\n",
        "            source_cla_dl = None\n",
        "            target_cla_dl = None\n",
        "        else:\n",
        "            # Define C-A Dataloader\n",
        "            source_cla_dl = create_class_aware_dataloader(source_filt_ds, classes)\n",
        "            target_cla_dl = create_class_aware_dataloader(target_filt_ds, classes)\n",
        "\n",
        "        train_metrics = training_step_improvements(model, source_tr_dl, source_cla_dl, target_cla_dl, optimizer, classes, kpca, device)\n",
        "\n",
        "        test_metrics = test_step_can(model, target_ts_dl, torch.nn.CrossEntropyLoss(), device)\n",
        "        \n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Log to WandB\n",
        "        metrics = {**train_metrics,\n",
        "                   'train/filtered_classes': len(classes),\n",
        "                   'train/clustering_acc': clustering_acc,\n",
        "                   **test_metrics}\n",
        "        wandb.log(metrics)\n",
        "\n",
        "        train_loss = train_metrics['train/train_loss']\n",
        "        train_ce = train_metrics['train/train_ce']\n",
        "        train_cdd = train_metrics['train/train_cdd']\n",
        "        train_acc = train_metrics['train/train_acc']\n",
        "\n",
        "        test_loss = test_metrics['test/test_loss']\n",
        "        test_acc = test_metrics['test/test_acc']\n",
        "\n",
        "        new_best = False\n",
        "        if e == 0 or best_acc < test_acc:\n",
        "            best_acc = test_acc\n",
        "            best_loss = test_loss\n",
        "            best_model = copy.deepcopy(model)\n",
        "            new_best = True\n",
        "\n",
        "            # Save new best weights\n",
        "            if save:\n",
        "                save_weights(e, model, optimizer, test_loss, f'/content/drive/MyDrive/weights/ResNet18CAN_{experiment}.pth', scheduler)\n",
        "                artifact = wandb.Artifact(f'ResNet18CAN_{experiment}', type='model', metadata={**wandb_run.config, **metrics})\n",
        "                artifact.add_file(f'/content/drive/MyDrive/weights/ResNet18CAN_{experiment}.pth')\n",
        "                wandb_run.log_artifact(artifact)\n",
        "\n",
        "        print('\\n Epoch: {:d}'.format(e + 1))\n",
        "        print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_acc))\n",
        "        print('\\t Training CE {:.5f}, Training CDD {:.5f}'.format(train_ce, train_cdd))\n",
        "        print('\\t Training filtered classes {}'.format(len(classes)))\n",
        "        print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_acc))\n",
        "        if new_best:\n",
        "            print(\"\\t New best weights saved\")\n",
        "        print('-----------------------------------------------------')\n",
        "\n",
        "    visualize(best_model, target_ts_dl, wandb_run)\n",
        "    wandb.summary[\"test_best_loss\"] = best_loss\n",
        "    wandb.summary[\"test_best_accuracy\"] = best_acc\n",
        "    wandb.finish()\n",
        "\n",
        "    del source_tr_static\n",
        "    del target_ts_static\n",
        "    del target_tr_static\n",
        "\n",
        "    print('\\t BEST Test loss {:.5f}, Test accuracy {:.2f}'.format(best_loss, best_acc))\n",
        "\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMV_--t2nL1M"
      },
      "source": [
        "### Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giuU5bwv7Skw"
      },
      "source": [
        "Notice that since we are using Adam optimizer we expect faster convergence, therefore we train the model for 30 epochs instead of 60 as in vanilla CAN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujMOWmHCnNtE"
      },
      "outputs": [],
      "source": [
        "conf = wandb.init(\n",
        "    project=\"DL2022_229356_229298\",\n",
        "    entity=\"229356_229298\",\n",
        "    name=\"P_to_R_improvements\",\n",
        "    config={\n",
        "        \"model\": \"ResNet18Improvements\",\n",
        "        \"trained-on\": \"Source + Target unsupervised\",\n",
        "        \"epochs\": 30,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"optimizer\": \"adam\",\n",
        "        \"lr\": 1e-3,\n",
        "        \"D_0\": 0.1,\n",
        "        \"N_0\": DEFAULT_N_0\n",
        "    }\n",
        ")\n",
        "\n",
        "best_model = training_loop_improvements(train_products, test_real, train_real, device, conf, save=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVUMxqRR_lYW"
      },
      "source": [
        "|    class | precision | recall| f1-score | support |\n",
        "|---|---|---|---|---|\n",
        "|            backpack    |   0.68 |     1.00 |     0.81 |       17|\n",
        "|            bookcase    |   0.85 |     0.85 |     0.85 |       20|\n",
        "|            car jack    |   0.83 |     1.00 |     0.91 |       15|\n",
        "|                comb    |   0.84 |     0.73 |     0.78 |       22|\n",
        "|               crown    |   0.91 |     1.00 |     0.95 |       21|\n",
        "|        file cabinet    |   0.68 |     0.68 |     0.68 |       22|\n",
        "|           flat iron    |   0.75 |     0.94 |     0.83 |       16|\n",
        "|     game controller    |   0.84 |     0.76 |     0.80 |       21|\n",
        "|             glasses    |   1.00 |     0.95 |     0.97 |       19|\n",
        "|          helicopter    |   0.89 |     0.89 |     0.89 |       19|\n",
        "|          ice skates    |   0.78 |     0.67 |     0.72 |       21|\n",
        "|         letter tray    |   0.72 |     0.78 |     0.75 |       27|\n",
        "|             monitor    |   0.95 |     0.95 |     0.95 |       20|\n",
        "|                 mug    |   1.00 |     0.88 |     0.93 |       24|\n",
        "|      network switch    |   0.88 |     0.82 |     0.85 |       17|\n",
        "| over-ear headphones    |   0.94 |     0.94 |     0.94 |       17|\n",
        "|                 pen    |   0.81 |     0.76 |     0.79 |       17|\n",
        "|               purse    |   0.76 |     0.57 |     0.65 |       23|\n",
        "|         stand mixer    |   0.88 |     1.00 |     0.93 |       21|\n",
        "|            stroller    |   1.00 |     0.90 |     0.95 |       21|\n",
        "| ||||\n",
        "|            accuracy    |        |          |     0.84 |      400|\n",
        "|           macro avg    |   0.85 |     0.85 |     0.85 |      400|\n",
        "|        weighted avg    |   0.85 |     0.84 |     0.84 |      400|\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5rkcF4HPhN9"
      },
      "outputs": [],
      "source": [
        "conf = wandb.init(\n",
        "    project=\"DL2022_229356_229298\",\n",
        "    entity=\"229356_229298\",\n",
        "    name=\"R_to_P_improvements\",\n",
        "    config={\n",
        "        \"model\": \"ResNet18Improvements\",\n",
        "        \"trained-on\": \"Source + Target unsupervised\",\n",
        "        \"epochs\": 30,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"optimizer\": \"adam\",\n",
        "        \"lr\": 1e-3,\n",
        "        \"D_0\": 0.1,\n",
        "        \"N_0\": DEFAULT_N_0\n",
        "    }\n",
        ")\n",
        "\n",
        "best_model = training_loop_improvements(train_real, test_products, train_products, device, conf, save=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxtQw4xDFdt4"
      },
      "source": [
        "| class                |precision |   recall | f1-score |  support|\n",
        "|---|---|---|---|---|\n",
        "|            backpack  |     0.94 |     1.00 |     0.97 |       29|\n",
        "|            bookcase  |     0.87 |     0.95 |     0.91 |       21|\n",
        "|            car jack  |     1.00 |     0.94 |     0.97 |       17|\n",
        "|                comb  |     0.86 |     1.00 |     0.93 |       19|\n",
        "|               crown  |     1.00 |     1.00 |     1.00 |       20|\n",
        "|        file cabinet  |     1.00 |     0.83 |     0.91 |       18|\n",
        "|           flat iron  |     0.88 |     0.88 |     0.88 |       16|\n",
        "|     game controller  |     1.00 |     0.96 |     0.98 |       24|\n",
        "|             glasses  |     1.00 |     1.00 |     1.00 |       19|\n",
        "|          helicopter  |     0.94 |     1.00 |     0.97 |       17|\n",
        "|          ice skates  |     1.00 |     0.95 |     0.97 |       19|\n",
        "|         letter tray  |     0.94 |     1.00 |     0.97 |       16|\n",
        "|             monitor  |     0.95 |     0.95 |     0.95 |       20|\n",
        "|                 mug  |     1.00 |     1.00 |     1.00 |       17|\n",
        "|      network switch  |     1.00 |     0.96 |     0.98 |       24|\n",
        "| over-ear headphones  |     0.88 |     1.00 |     0.94 |       15|\n",
        "|                 pen  |     0.89 |     0.83 |     0.86 |       29|\n",
        "|               purse  |     1.00 |     0.86 |     0.92 |       21|\n",
        "|         stand mixer  |     1.00 |     1.00 |     1.00 |       19|\n",
        "|            stroller  |     0.95 |     1.00 |     0.98 |       20|\n",
        "|                      |          |          |          |         |   \n",
        "|            accuracy  |          |          |     0.95 |      400|\n",
        "|           macro avg  |     0.96 |     0.96 |     0.95 |      400|\n",
        "|        weighted avg  |     0.95 |     0.95 |     0.95 |      400|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNmuJDKZnW1b"
      },
      "source": [
        "### Observations\n",
        "\n",
        "Thanks to the above mentioned improvements we were able to substantially increase the performances of the Contrastive Adaptation Network reaching 84.5% accuracy on $P → R$ task and 95.2% accuracy on $R → P$ task, which is the same as the upper bound accuracy we computed on Products test set. Thus the **gain** obtained is of 10% on $P → R$ task and of 2.7% $R → P$ task.\n",
        "\n",
        "As we can see in the plots below not only the training required less epochs but the convergence to the optimum was steeper so that the best accuracy obtained in vanilla CAN was already outperformed by epoch 5 in $P → R$ task and epoch 6 in $R → P$ task, whereas for vanilla CAN it took 60 iterations to reach those performaces.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1YlWZRu4rwxNqqeNoPQRj_RpdotprlHJz\" width=\"500px\">\n",
        "\n",
        "We can notice that classes are compact and well separated even though some outliers remain. Thus intra-class compactness is higher and inter-class margin is larger compared to standard CAN implementation. This behaviour is a symptom of better minimization of the Contrastive Domain Discrepancy thanks to both Adam and the computation of the CDD on features with reduced dimensionality. We believe that by implementing Kernel PCA dimensionality reduction, the CDD computed was less subject to noise and therefore more representative of the actual distribution of the data, leading to better domain adaptation.\n",
        "\n",
        "<div>P->R t-SNE representation on the left and R->P on the right.<br>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1E2AErYYu_1qK3C5WtGHtYQNXv_tz64CG\" width=\"500px\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1wK2eUv9pLvmbzdAvsZjB6UanFCbGT909\" width=\"500px\">\n",
        "</div>\n",
        "\n",
        "In addition we can see that the CDD loss minimization does not stall nor oscillates around a constant value like in standard CAN. In our improved CAN the CDD loss has a smoother behaviour and its value is steadily minimized througout the training.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1AW516FV6T8gkynfjFkkNpVK-74vRqDI6\" width=\"500px\">\n",
        "\n",
        "The better behaviour of the CDD has a positive impact on the accuracy of the predicted target labels, as we can see in the plot below.\n",
        "Although the clustering accuracy does not increase smoothly as the CDD is minimzed, we can see that it reaches a higher value earlier during the training, w.r.t. vanilla CAN. We believe that this creates a virtuous circle since more accurate pseudo-labels for the target set allow to compute a more precise CDD, thus the model's features are more representative of the samples, leading to more accurate clustering.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1fOMdHK919yz5nbl9gL7oAWKicq7e1DKc\" width=\"500px\">\n",
        "\n",
        "For the same reasons we observe, particularly for the task $P → R$, that the number of classes used for the computation of the CDD increases to an amount near to the maximum in less iterations. This means that the compacteness of the feature representations (intra-class domain discrepancy) for the same class is higher, leading to more confident target-label predictions. Therefore less samples are excluded by the filtering and more (and precise) information is used to compute the CDD.\n",
        "\n",
        "We can speculate that the reason why in $P → R$ task the number of remaining classes never reaches the maximum could be related to the need of finding better $D_0$ and $N_0$ hyperparameters as we just used similar values to the ones proposed in the original article by the authors. It could be also that the model fails in compacting enough the worst performing class (maybe \"file cabinet\" as we can se from the confusion matrix below or the classification report in $P → R$ training section), therefore those parameters, in particular $N_0$, are too strict for it, while they are fine for the majority of the classes.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Na0vbxkQ-99QEIjUTkvVXbMXfuVD35Z6\" width=\"500px\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Jo6L-T0nAFVoGGhezNzHtBMYMxmqh85u\" width=\"700px\">\n",
        "\n",
        "In the end we can appreciate the final **gain** obtained in our improved version of CAN with respect to the baseline.\n",
        "\n",
        "| Method | Acc P -> R (%) | Acc R -> P (%)|\n",
        "|---|---|---|\n",
        "| *Upper bound*         |  *92.0*  |  *95.0*  |\n",
        "| Baseline              |   74.5   |   92.5   |\n",
        "| CAN                   |   79.0   |   93.5   |\n",
        "| **Gain CAN**          | **4.5**  |  **1.0** |\n",
        "| CAN-Improved          |   84.5   |   95.2   |\n",
        "| **Gain CAN-Improved** | **10.0** |  **2.7** |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWrck8Phic1n"
      },
      "source": [
        "## Ablation studies\n",
        "\n",
        "With the following experiments we tried to understand what is the impact on the performance of CAN by only applying dimensionality reduction, keeping the original SGD optimizer. To get meaningful results, we needed to tweak slightly $D_0$ hyperparameter to $0.15$, increasing the threshold for choosing to filter out a sample for the CDD loss computation.\n",
        "\n",
        "As we can see below, in $P → R$ task there is an improvemnt in the overall accuracy from $0.79$ to $0.82$. Furthermore, as expected, the CDD loss has a smoother behaviour compared to the vanilla CAN, even though it stalls at an higher value. Despite this, the test accuracy continues to increase. \n",
        "\n",
        "We can also see that, thanks to the more compact feature representation, the clustering accuracy is higher than vanilla CAN, allowing the model to exploit a more accurate pseudo-labelling for the computation of the CDD loss.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1yeg6oGzMWRoNqLpVTUxEAjfCX6aCXMAF\" width=\"500px\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1dc1EsouaCWbdUaJrRYipk_hCp3kVled4\" width=\"500px\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1d_TOdHF3donxE2DsUwZaT5EaEt8RzCq5\" width=\"500px\">\n",
        "\n",
        "In $R → P$ task, instead, we do not get an improvement in the best test accuracy value, however similar behaviours both in the clustering and in the CDD loss curves are observed. \n",
        "\n",
        "Comparing those results with the full improved version we discussed previously, we can observe that Adam optimizer has a huge effect both in reaching higher accuracy and smoother loss minimization thanks to its adaptive learning rate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFrPiNVw2gKL"
      },
      "outputs": [],
      "source": [
        "conf = wandb.init(\n",
        "    project=\"DL2022_229356_229298\",\n",
        "    entity=\"229356_229298\",\n",
        "    name=\"P_to_R_ablation_dim_red\",\n",
        "    config={\n",
        "        \"model\": \"ResNet18ImprovementsAblationDimRed\",\n",
        "        \"trained-on\": \"Source + Target unsupervised\",\n",
        "        \"epochs\": 60,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"optimizer\": \"sgd\",\n",
        "        \"lr\": 1e-2,\n",
        "        \"D_0\": 0.15,\n",
        "        \"N_0\": DEFAULT_N_0\n",
        "    }\n",
        ")\n",
        "\n",
        "best_model = training_loop_improvements(train_products, test_real, train_real, device, conf, save=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwT926Ag2g9M"
      },
      "source": [
        "|       class        | precision |   recall  |f1-score  | support|\n",
        "|---|---|---|---|---|\n",
        "|           backpack |      0.62 |     0.88  |    0.73  |      17|\n",
        "|           bookcase |      0.75 |     0.90  |    0.82  |      20|\n",
        "|           car jack |      0.76 |     0.87  |    0.81  |      15|\n",
        "|               comb |      1.00 |     0.73  |    0.84  |      22|\n",
        "|              crown |      0.83 |     0.90  |    0.86  |      21|\n",
        "|       file cabinet |      0.76 |     0.59  |    0.67  |      22|\n",
        "|          flat iron |      0.82 |     0.88  |    0.85  |      16|\n",
        "|    game controller |      1.00 |     0.76  |    0.86  |      21|\n",
        "|            glasses |      0.93 |     0.68  |    0.79  |      19|\n",
        "|         helicopter |      0.82 |     0.95  |    0.88  |      19|\n",
        "|         ice skates |      0.80 |     0.76  |    0.78  |      21|\n",
        "|        letter tray |      0.68 |     0.78  |    0.72  |      27|\n",
        "|            monitor |      0.84 |     0.80  |    0.82  |      20|\n",
        "|                mug |      0.91 |     0.88  |    0.89  |      24|\n",
        "|     network switch |      0.80 |     0.71  |    0.75  |      17|\n",
        "|over-ear headphones |      0.89 |     1.00  |    0.94  |      17|\n",
        "|                pen |      0.94 |     0.94  |    0.94  |      17|\n",
        "|           stroller |      0.89 |     0.76  |    0.82  |      21|\n",
        "|              purse |      0.65 |     0.74  |    0.69  |      23|\n",
        "|        stand mixer |      0.91 |     0.95  |    0.93  |      21|\n",
        "| | | | |\n",
        "|           accuracy |           |           |    0.82  |     400|\n",
        "|          macro avg |      0.83 |     0.82  |    0.82  |     400|\n",
        "|       weighted avg |      0.83 |     0.82  |    0.82  |     400|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-3DlkFcf8DH"
      },
      "outputs": [],
      "source": [
        "conf = wandb.init(\n",
        "    project=\"DL2022_229356_229298\",\n",
        "    entity=\"229356_229298\",\n",
        "    name=\"R_to_P_ablation_dim_red\",\n",
        "    config={\n",
        "        \"model\": \"ResNet18ImprovementsAblationDimRed\",\n",
        "        \"trained-on\": \"Source + Target unsupervised\",\n",
        "        \"epochs\": 60,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"optimizer\": \"sgd\",\n",
        "        \"lr\": 1e-2,\n",
        "        \"D_0\": 0.15,\n",
        "        \"N_0\": DEFAULT_N_0\n",
        "    }\n",
        ")\n",
        "\n",
        "best_model = training_loop_improvements(train_real, test_products, train_products, device, conf, save=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTWOnM3hh-i6"
      },
      "source": [
        "|                    | precision |   recall|  f1-score|   support|\n",
        "|---|---|---|---|---|\n",
        "|           backpack |      0.93 |     0.97|      0.95|        29|\n",
        "|           bookcase |      0.74 |     0.95|      0.83|        21|\n",
        "|           car jack |      0.93 |     0.76|      0.84|        17|\n",
        "|               comb |      0.86 |     0.95|      0.90|        19|\n",
        "|              crown |      1.00 |     1.00|      1.00|        20|\n",
        "|       file cabinet |      0.93 |     0.78|      0.85|        18|\n",
        "|          flat iron |      0.79 |     0.94|      0.86|        16|\n",
        "|    game controller |      1.00 |     0.92|      0.96|        24|\n",
        "|            glasses |      0.95 |     0.95|      0.95|        19|\n",
        "|         helicopter |      0.81 |     1.00|      0.89|        17|\n",
        "|         ice skates |      1.00 |     0.95|      0.97|        19|\n",
        "|        letter tray |      0.92 |     0.75|      0.83|        16|\n",
        "|            monitor |      1.00 |     0.95|      0.97|        20|\n",
        "|                mug |      1.00 |     1.00|      1.00|        17|\n",
        "|     network switch |      0.92 |     1.00|      0.96|        24|\n",
        "|over-ear headphones |      0.88 |     1.00|      0.94|        15|\n",
        "|                pen |      0.92 |     0.83|      0.87|        29|\n",
        "|              purse |      0.94 |     0.81|      0.87|        21|\n",
        "|        stand mixer |      0.94 |     0.89|      0.92|        19|\n",
        "|           stroller |      1.00 |     1.00|      1.00|        20|\n",
        "| | | | | |\n",
        "|           accuracy |           |         |      0.92|       400|\n",
        "|          macro avg |      0.92 |     0.92|      0.92|       400|\n",
        "|       weighted avg |      0.93 |     0.92|      0.92|       400|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIuar9FB0OnV"
      },
      "source": [
        "## Experiment with tuned hyperparameters\n",
        "In order to have more comparable results in the experiments discussed previously we avoided, as much as possible, tweaking hyper-parameters $D_0$ and $N_0$. \n",
        "\n",
        "However, focusing on the most difficult task $P → R$, we empirically found that setting $N_0=0.2$ the improved version with Adam optimizer and dimensionality reduction, was able to reach even higher test accuracy, with a best value of $0.88$. Similar behaviour is observed in the CDD loss and accuracy curves. The main diffence is that all 20 classes are used for CDD loss computation since the beginning of the training. In other words, no class filtering is performed.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1c_wo4dvn3m7QZeo_idPhFe62nleTVBae\" width=\"500px\">\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1rbZaCV1YfGyNDzQ2L4W_MraNPzwK5NUH\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWVY6JAq4rGd"
      },
      "source": [
        "## Conclusions\n",
        "With this Notebook we showed the three major steps that led us to solve the task. First by using no domain adaptations techniques then, by appling the Contrastive Adaptation Network and, in the end, our improved version of it. \n",
        "\n",
        "During the developement of the project we encountered several challenges expecially due to performance issues, but also in understanding how to implement CAN and its training procedure due to the lack of details in the original paper. \n",
        "\n",
        "In the last section we showed how our improved version of CAN has been able to be competitive against its vanilla implementation and how we have been able to address the domain gap between Real World and Products datasets. Moreover, results are comparable with those provided by Adaptiope paper on CAN, although should be taken with a grain of salt since we are dealing with a subset of the original dataset.\n",
        "\n",
        "[Here](https://wandb.ai/229356_229298/DL2022_229356_229298) you can find our Weight and Biases project with all the plots we showed and additional metrics and informations, like some wrongly predicted images.\n",
        "\n",
        "Below we can find a recap of the test accuracy results for each one of the methods we performed, with gain in accuracy with respect to the baseline in bold.\n",
        "\n",
        "| Method | Acc P -> R (%) | Acc R -> P (%)|\n",
        "|---|---|---|\n",
        "| *Upper bound*           |  *92.0*  |  *95.0*  |\n",
        "| Baseline                |   74.5   |   92.5   |\n",
        "| CAN                     |   79.0   |   93.5   |\n",
        "| **Gain CAN**            | **4.5**  |  **1.0** |\n",
        "| CAN-Improved            |   84.5   |   95.2   |\n",
        "| CAN-Improved-tuned |   87.8  |      |\n",
        "| **Gain CAN-Improved**   | **13.3** |  **2.7** |\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
